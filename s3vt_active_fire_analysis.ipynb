{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentinel 3 Validation Team Australia - Analysis of Hotspots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from netCDF4 import Dataset\n",
    "import datetime as dt\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "import shapely.speedups\n",
    "shapely.speedups.enable()\n",
    "#import xmltodict\n",
    "import yaml\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.geometry import Point, LineString\n",
    "from geopy.distance import distance\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import folium\n",
    "import json\n",
    "import joblib\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import time\n",
    "#from dask.distributed import Client\n",
    "\n",
    "# If you want Dask to set itself up on your personal computer\n",
    "#client = Client(processes=False)\n",
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(level=logging.INFO)\n",
    "logging.basicConfig(filename='notebook.log',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open configuration file and read parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'config.yaml') as file:\n",
    "    configuration =  yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in configuration['configurations']:\n",
    "    username = config['username']\n",
    "    password = config['password']\n",
    "    url = config['url']\n",
    "    aoi = config['aoi']\n",
    "    awss3bucket = config['awss3bucket']\n",
    "    awskeyid = config['awskeyid']\n",
    "    awskeypass = config['awskeypass']\n",
    "    hotspotslogin = config['hotspots_login']\n",
    "    hotspotspassword = config['hotspots_password']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_satellite_swaths(configuration, start, period, solar_day):\n",
    "    \"\"\"\n",
    "    Function to determine the common imaging footprint of a pair of sensors\n",
    "    \n",
    "    Returns the imaging footprints of a pair of sensors for a period from a starting datetime\n",
    "    \"\"\"\n",
    "    output = Path('output')\n",
    "    dirpath = Path.joinpath(output, solar_day)\n",
    "    \n",
    "    if (dirpath.exists()):\n",
    "        logger.info(str(solar_day)+\" exists - skipping swath generation\")\n",
    "        success = True\n",
    "    else:\n",
    "        dirpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "        try:\n",
    "            logger.info('Generating swaths '+str(['python', 'swathpredict.py', '--configuration', configuration, '--start', start, '--period', period, '--output_path', str(dirpath)])\n",
    ")\n",
    "            subprocess.call(['python', 'swathpredict.py', '--configuration', configuration, '--start', start, '--period', period, '--output_path', str(dirpath)])\n",
    "            \n",
    "            success = True\n",
    "        except:\n",
    "            success = False\n",
    "            logger.info('Swath generation failed')\n",
    "    \n",
    "    return(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_swath_intersect(satsensorsA, satsensorsB, solar_day):\n",
    "    \"\"\"\n",
    "    Function intersect geometries of sensors for a given solar day \n",
    "    \n",
    "    Returns intersection geometry\n",
    "    \"\"\"\n",
    "    logging.info(\"Running intersection for \"+str(satsensorsA)+' '+str(satsensorsB))\n",
    "    satsensorsA = [w.replace(' ', '_') for w in satsensorsA]\n",
    "    satsensorsB = [w.replace(' ', '_') for w in satsensorsB]\n",
    "        \n",
    "    filesA = []\n",
    "    filesB = []\n",
    "    \n",
    "    output = Path('output')\n",
    "    dirpath = Path.joinpath(output, solar_day)\n",
    "    \n",
    "    for sat in satsensorsA:\n",
    "   \n",
    "        filesA.extend([f for f in os.listdir(str(dirpath)) if (sat in f )and ('swath.geojson' in f)])\n",
    "        \n",
    "    for sat in satsensorsB:\n",
    "    \n",
    "        filesB.extend([f for f in os.listdir(str(dirpath)) if sat in f and 'swath.geojson' in f])\n",
    "    \n",
    "    gpdlistA = []\n",
    "    for file in filesA:\n",
    "        df = gpd.read_file(Path.joinpath(dirpath, file))\n",
    "        gpdlistA.append(df)\n",
    "    gpdlistB = []\n",
    "    for file in filesB:\n",
    "        df = gpd.read_file(Path.joinpath(dirpath, file))\n",
    "        gpdlistB.append(df)        \n",
    "    return(pd.concat(gpdlistA),pd.concat(gpdlistB) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar_day_start_stop_period(longitude_east, longitude_west, solar_day):\n",
    "    \"\"\"\n",
    "    Function solar day start time from longitude and solar day in utc \n",
    "    \n",
    "    Returns datetime start stop in utc and period between in minutes\n",
    "    \"\"\"\n",
    "    # Solar day time relative to UTC and local longitude\n",
    "    SECONDS_PER_DEGREE = 240\n",
    "    # Offset for eastern limb\n",
    "    offset_seconds_east = int(longitude_east * SECONDS_PER_DEGREE)\n",
    "    offset_seconds_east = np.timedelta64(offset_seconds_east, 's')\n",
    "    # offset for wester limb\n",
    "    offset_seconds_west = int(longitude_west * SECONDS_PER_DEGREE)\n",
    "    offset_seconds_west = np.timedelta64(offset_seconds_west, 's')\n",
    "    # time between two limbs\n",
    "    offset_day = np.timedelta64(1440, 'm') + abs(offset_seconds_east - offset_seconds_west)\n",
    "    #ten_am_crossing_adjustment = np.timedelta64(120, 'm')\n",
    "    # Solar day start at eastern limb\n",
    "    solar_day_start_utc = (np.datetime64(solar_day) - offset_seconds_east ).astype(datetime)\n",
    "    # Solar day finish at western limb\n",
    "    solar_day_finish_utc = ((np.datetime64(solar_day)+offset_day)  - offset_seconds_east ).astype(datetime)\n",
    "    # Duration of solar day\n",
    "    solar_day_duration = np.timedelta64((solar_day_finish_utc - solar_day_start_utc), 'm' )\n",
    "    \n",
    "    return(solar_day_start_utc, solar_day_finish_utc , solar_day_duration.astype(datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar_day(utc, longitude):\n",
    "    \"\"\"\n",
    "    Function solar day for a given UTC time and longitude input\n",
    "    \n",
    "    Returns datetime object representing solar day\n",
    "    \"\"\"\n",
    "    SECONDS_PER_DEGREE = 240\n",
    "    offset_seconds = int(longitude * SECONDS_PER_DEGREE)\n",
    "    offset = np.timedelta64(offset_seconds, 's')\n",
    "    return (np.datetime64(utc) + offset).astype(datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ckdnearest(gdA, gdB):\n",
    "    \"\"\"\n",
    "    Function to find points in \"B\" nearest to \"A\" geopandas dataframe\n",
    "    \n",
    "    Returns geopandas dataframe with records representing matches\n",
    "    \"\"\"\n",
    "    nA = np.array(list(zip(gdA.geometry.x, gdA.geometry.y)) )\n",
    "    nB = np.array(list(zip(gdB.geometry.x, gdB.geometry.y)) )\n",
    "    btree = cKDTree(nB)\n",
    "    dist, idx = btree.query(nA, k=1)\n",
    "    gdf = gpd.GeoDataFrame(pd.concat(\n",
    "        [gdA.reset_index(drop=True), gdB.loc[idx].reset_index(drop=True).add_prefix('2_'),\n",
    "         pd.Series(dist, name='dist')], axis=1))\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial and temporal area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the area of interest\n",
    "# Scenario 1 - all of Australia\n",
    "#westlon = 110.0\n",
    "#southlat = -50.0\n",
    "#eastlon = 160.0\n",
    "#northlat = -10.0\n",
    "#bbox = (westlon, southlat, eastlon, northlat)\n",
    "#start_date = '2019-11-01'\n",
    "#end_date = '2020-05-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 2 - NSW AOI determine by NSW RFS burnt area Isochrons\n",
    "westlon = 147\n",
    "southlat = -38\n",
    "eastlon = 154\n",
    "northlat = -27\n",
    "bbox = (westlon, southlat, eastlon, northlat)\n",
    "start_date = '2019-12-15'\n",
    "end_date = '2020-01-15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Hotpots files from AWS S3 to local "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you're within the s3vt repository s3vt/ \n",
    "if os.path.exists(\"DEAHotspots_hotspots.geojson\"):\n",
    "    print('exists')\n",
    "else:\n",
    "    ! aws s3 cp s3://s3vtaustralia/DEAHotspots.geojson DEAHotspots_hotspots.geojson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"s3vt_hotspots.geojson\"):\n",
    "    print('exists')\n",
    "else:\n",
    "    ! aws s3 cp s3://s3vtaustralia/s3vt_hotspots.geojson s3vt_hotspots.geojson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Hotspots to GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf = gpd.read_file('DEAHotspots_hotspots.geojson', bbox=bbox)\n",
    "hotspots_gdf['datetime'] = pd.to_datetime(hotspots_gdf['datetime'])\n",
    "hotspots_gdf['solar_day'] = pd.to_datetime(hotspots_gdf['solar_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots = gpd.read_file('s3vt_hotspots.geojson', bbox=bbox)\n",
    "s3vthotspots['datetime'] = pd.to_datetime(s3vthotspots['date'])\n",
    "s3vthotspots['solar_day'] = pd.to_datetime(s3vthotspots['solar_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up S3 Hotspots to allow single GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots.rename(columns={'F1_Fire_pixel_radiance':'power'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots = s3vthotspots.drop(['FRP_MWIR', 'FRP_SWIR', 'FRP_uncertainty_MWIR',\n",
    "       'FRP_uncertainty_SWIR', 'Glint_angle', 'IFOV_area', 'Radiance_window',\n",
    "       'S7_Fire_pixel_radiance', 'TCWV', 'classification',  'i',\n",
    "       'j', 'n_SWIR_fire', 'n_cloud', 'n_water',\n",
    "       'n_window', 'time', 'transmittance_MWIR', 'transmittance_SWIR',\n",
    "       'used_channel', 'date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots['satellite_sensor_product'] = s3vthotspots['satellite']+'_'+s3vthotspots['sensor']+'_ESA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf['satellite_sensor_product'] = hotspots_gdf['satellite']+'_'+hotspots_gdf['sensor']+'_'+hotspots_gdf['product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf = hotspots_gdf.drop(['product'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf = pd.concat([hotspots_gdf, s3vthotspots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the S3 geodataframe object\n",
    "s3vthotspots = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating doesn't update the index automatically\n",
    "hotspots_gdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index by solar day to enable groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf =  hotspots_gdf.set_index(pd.DatetimeIndex(hotspots_gdf.solar_day.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal subset (by solar day) to enable rapid testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_prefix = start_date+'_'+end_date\n",
    "hotspots_gdf = hotspots_gdf.loc[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf.loc[start_date:end_date]['datetime'].min(), hotspots_gdf.loc[start_date:end_date]['datetime'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map plot hotspots\n",
    "hotspots_gdf.plot(column='satellite_sensor_product', legend=True, legend_kwds={'loc': 'upper right'}, figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run comparison matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hotspots(productA, hotspots_gdf):\n",
    "    \"\"\"\n",
    "    Function compares sensor swaths of productA to those of sensors in hotspots_gdf -subsets hotspots to\n",
    "    common imaged area and determines points nearest to productA from hotspots_gdf. Computes distance between points.\n",
    "    \n",
    "    Returns dataframe containing matched pairs between the two sets of hotspots as w\n",
    "    \"\"\"\n",
    "    appended_dataframe = []\n",
    "    nearest_points_list = []\n",
    "    satellite_sensor_product_intersections = {}\n",
    "    for productB in set(hotspots_gdf['satellite_sensor_product']):\n",
    "\n",
    "        gdfA = hotspots_gdf[(hotspots_gdf['satellite_sensor_product'] == productA)]       \n",
    "        gdfB = hotspots_gdf[(hotspots_gdf['satellite_sensor_product'] == productB)]       \n",
    "\n",
    "        # For each solar day group in gdfA\n",
    "        for Aname, Agroup in gdfA.resample('D', on='solar_day'):\n",
    "\n",
    "            minutctime, maxutctime, deltautctime = solar_day_start_stop_period(eastlon, westlon, Aname)\n",
    "\n",
    "            # For each solar day group in gdfB\n",
    "            for Bname, Bgroup in gdfB.resample('D', on='solar_day'):      \n",
    "\n",
    "                # Do where the solar days are the same in gdfA and B\n",
    "                if (Aname == Bname):\n",
    "                    logger.info(productA+' '+productB)\n",
    "\n",
    "                    logger.info(str(Aname)+' '+str(minutctime)+' '+str(minutctime)+' '+str(deltautctime))\n",
    "\n",
    "                    # Generate the GeoJSON for each satellite in s3vtconfig.yaml\n",
    "\n",
    "                    get_satellite_swaths('s3vtconfig.yaml', minutctime.strftime(\"%Y-%m-%dT%H:%M:%SZ\"), str(int(deltautctime.total_seconds()/60)), str(Aname.date()))\n",
    "\n",
    "                    # Geostationary satellites need an exception\n",
    "                    if not (('AHI' in [productA, productB]) or ('INS1' in [productA, productB])):\n",
    "\n",
    "                        # Include a try except to counteract failures where swath intersect fails\n",
    "                        try:\n",
    "\n",
    "                            # Get geometries for satellite sensors in gpdA and gpdB\n",
    "                            gpd1, gpd2 = pairwise_swath_intersect(set(Agroup['satellite']), set(Bgroup['satellite']), str(Aname.date()))\n",
    "\n",
    "                            # Union before intersect\n",
    "                            gpd1 = gpd1.unary_union\n",
    "                            gpd2 = gpd2.unary_union\n",
    "\n",
    "                            # Intersect geometries\n",
    "                            intersection = gpd1.intersection(gpd2)\n",
    "                            logger.info(str(intersection))\n",
    "\n",
    "\n",
    "                            if intersection == None:\n",
    "                                logger.info(\"Intersection is None\")\n",
    "                            else:\n",
    "                                logger.info(\"Intersection successful\")\n",
    "                            # Use intersection results to subset points (compare common imaged area)\n",
    "\n",
    "                            logger.info(\"Before intersection \"+str(Aname)+' '+str(Agroup['satellite_sensor_product'].count())+' '+str(Bgroup['satellite_sensor_product'].count()))\n",
    "\n",
    "                            pip_mask = Agroup.within(intersection)\n",
    "                            Agroup = Agroup.loc[pip_mask]                                \n",
    "                            Agroup.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                            pip_mask = Bgroup.within(intersection)\n",
    "                            Bgroup = Bgroup.loc[pip_mask]\n",
    "                            Bgroup.reset_index(drop=True, inplace=True)\n",
    "                            logger.info(\"After intersection \"+str(Aname)+' '+str(Agroup['satellite_sensor_product'].count())+' '+str(Bgroup['satellite_sensor_product'].count()))\n",
    "\n",
    "                            if (Agroup['solar_day'].count() == 0) or (Bgroup['solar_day'].count() == 0):\n",
    "                                logger.info(\"Nothing to input to ckdnearest\")\n",
    "\n",
    "                            per_solarday_nearest_hotspots = ckdnearest(Agroup , Bgroup)\n",
    "\n",
    "                            appended_dataframe.append(per_solarday_nearest_hotspots)\n",
    "\n",
    "                        except:\n",
    "                            logger.info('Skipping')\n",
    "                    else:\n",
    "                        # Himawari AHI or INS1 geostationary case\n",
    "                        # A better approach here is to check if either has a swath available\n",
    "                        # If not - defer to the intersection of the one with a geometry\n",
    "                        # TODO - improve for Himawari\n",
    "                        try:\n",
    "                            per_solarday_nearest_hotspots = ckdnearest(Agroup.reset_index(drop=True, inplace=True), Bgroup.reset_index(drop=True, inplace=True))\n",
    "                            print(len(appended_dataframe))\n",
    "                            appended_dataframe.append(per_solarday_nearest_hotspots)\n",
    "                            #per_solarday_nearest_hotspots.to_file(productA+'.geojson')\n",
    "                        except:\n",
    "                            logger.info('Skipping')\n",
    "        nearest_points = gpd.GeoDataFrame( pd.concat( appended_dataframe, ignore_index=True) )\n",
    "        nearest_points.reset_index(inplace=True, drop=True)\n",
    "        \n",
    "        outputfile = 'nearest_points.'+productA+'.csv'\n",
    "        nearest_points.to_csv(outputfile)\n",
    "        \n",
    "    return(productA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all hotspot sources to each other\n",
    "# Option 1 - sequential processing\n",
    "#for productA in set(hotspots_gdf['satellite_sensor_product']):\n",
    "#    compare_hotspots(productA, hotspots_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all hotspot sources to each other\n",
    "# Option 2 - parallel processing\n",
    "# Dask backend is having issues with memory leak to trying loky - limits options for outside memory but monthly partitions\n",
    "# should provide an ok work around\n",
    "\n",
    "nearest_points_list = []\n",
    "appended_dataframe = []\n",
    "satellite_sensor_product_intersections = {}\n",
    "with joblib.parallel_backend('loky'):\n",
    "    joblib.Parallel(verbose=100)(\n",
    "        joblib.delayed(compare_hotspots)(productA, hotspots_gdf)\n",
    "                       for productA in set(hotspots_gdf['satellite_sensor_product']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from shapely import wkt\n",
    "appended_dataframe = []\n",
    "for inputfile in glob.glob('nearest_points*.csv'):\n",
    "    df = pd.read_csv(inputfile)\n",
    "    geometry = df.geometry.apply(wkt.loads)\n",
    "    crs = 'epsg:4326'\n",
    "    #df['datetime'] = pd.to_datetime(hotspots_gdf['datetime'])\n",
    "    #df['solar_day'] = pd.to_datetime(hotspots_gdf['solar_day'])\n",
    "    appended_dataframe.append(gpd.GeoDataFrame(df, crs=crs, geometry=geometry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dask.dataframe as dd\n",
    "#nearest_points = dd.read_csv('nearest*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_points = gpd.GeoDataFrame(pd.concat(appended_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_points['datetime'] = pd.to_datetime(nearest_points['datetime'])\n",
    "nearest_points['solar_day'] = pd.to_datetime(nearest_points['solar_day'])\n",
    "nearest_points['2_datetime'] = pd.to_datetime(nearest_points['2_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metres distance between two points\n",
    "#nearest_points['dist_m'] = nearest_points.apply(lambda row: distance((row.latitude, row.longitude),(row['2_latitude'], row['2_longitude'])).meters, axis = 1)\n",
    "#nearest_points.compute(meta=(None, 'float64'))\n",
    "# Add time delta between points\n",
    "#nearest_points['timedelta'] = (abs(nearest_points['datetime'] - nearest_points['2_datetime']))\n",
    "#nearest_points['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#appended_dataframe = None\n",
    "\n",
    "# Add metres distance between two points\n",
    "nearest_points['dist_m'] = nearest_points.apply(lambda row: distance((row.latitude, row.longitude),(row['2_latitude'], row['2_longitude'])).meters, axis = 1)\n",
    "# Add time delta between points\n",
    "#nearest_points['timedelta'] = nearest_points.apply(lambda row: (abs(row['datetime'] - row['2_datetime'])), axis = 1)\n",
    "\n",
    "nearest_points['timedelta'] = (abs(nearest_points['datetime'] - nearest_points['2_datetime']))\n",
    "nearest_points['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output = comparison_prefix+'-'+'nearest_points.geojson'\n",
    "#nearest_points.to_file(output, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_points.set_index('solar_day', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "australia = gpd.GeoDataFrame.from_file('zip://vectors/TM_WORLD_BORDERS_SIMPL-0.3.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "australia = australia[australia.NAME=='Australia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING For testing intersect results for swath and hotspots\n",
    "\n",
    "satelliteA = 'SENTINEL_3B' # NOAA 19, NOAA 20, TERRA, AQUA, SENTINEL_3A, SENTINEL_3B\n",
    "satelliteB = 'TERRA'\n",
    "solar_date = '2020-01-04'\n",
    "gpd1, gpd2 = pairwise_swath_intersect([satelliteA], [satelliteB], solar_date)\n",
    "\n",
    "start_date = solar_date\n",
    "end_date = solar_date\n",
    "# uncomment below to examine data prior to intersect\n",
    "hotspots_sample = hotspots_gdf.loc[start_date:end_date]\n",
    "\n",
    "# uncomment below to examine data post intersect\n",
    "#nearest_sample = nearest_points[(nearest_points['satellite'] == satelliteA) & (nearest_points['2_satellite'] == satelliteB) ]\n",
    "#nearest_sample = pd.concat([nearest_sample, nearest_points[(nearest_points['satellite'] == satelliteB) & (nearest_points['2_satellite'] == satelliteA) ]])\n",
    "#hotspots_sample = nearest_sample.loc[start_date:end_date]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "gpd1.plot(ax=ax, facecolor='red');\n",
    "gpd2.plot(ax=ax, color='green');\n",
    "hotspots_sample[hotspots_sample['satellite'] == satelliteA].plot(ax=ax, color='tomato', markersize=2)\n",
    "hotspots_sample[hotspots_sample['satellite'] == satelliteB].plot(ax=ax, color='lime', markersize=2)\n",
    "\n",
    "gpd1int = gpd1.unary_union\n",
    "gpd2int = gpd2.unary_union\n",
    "\n",
    "intersection = gpd1int.intersection(gpd2int)\n",
    "\n",
    "#intersection = intersection[~intersection.is_empty]\n",
    "australia.plot(ax=ax, facecolor=\"None\", edgecolor='white')\n",
    "#gpd.GeoDataFrame(intersection).rename(columns={0:'geometry'}).set_geometry('geometry').plot(ax=ax, facecolor=\"None\", edgecolor='black', hatch=\"///\")\n",
    "gpd.GeoDataFrame(gpd.GeoSeries(intersection)).rename(columns={0:'geometry'}).set_geometry('geometry').plot(ax=ax, facecolor=\"None\", edgecolor='gray', hatch=\"///\")\n",
    "# Prepare for labels on GPD1\n",
    "gpd1['coords'] = gpd1['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "gpd1['coords'] = [coords[0] for coords in gpd1['coords']]\n",
    "for idx, row in gpd1.iterrows():\n",
    "    plt.annotate(s=row['Transit time                 :'], xy=row['coords'], horizontalalignment='center')\n",
    "# Plot it\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph it to confirm intersect of swath and hotspots (all won't match - not hotspots in the sea etc.)\n",
    "gpd1['Transit time                 :'] = pd.to_datetime(gpd1['Transit time                 :'])\n",
    "df = hotspots_sample[(hotspots_sample['satellite'] == satelliteA)].sort_values('datetime', ascending=True)\n",
    "dfswath = gpd1.sort_values('Transit time                 :', ascending=True)\n",
    "plt.plot(df['datetime'], df['datetime'], '*', color='blue', markersize=10)\n",
    "plt.plot(dfswath['Transit time                 :'], dfswath['Transit time                 :'], 'o', color='red', markersize=5)\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_points['geometry'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "# Co-occurrence metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of hotspot matches < 5000m\n",
    "numerator = pd.pivot_table(nearest_points[(nearest_points['dist_m'] < 5000)],values='count', index=['satellite_sensor_product'], columns=['2_satellite_sensor_product'], aggfunc={'count':len})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of hotspot matches - total\n",
    "denominator = pd.pivot_table(nearest_points,values='count', index=['2_satellite_sensor_product'], columns=['satellite_sensor_product'], aggfunc={'count':np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn styling for matrix\n",
    "denominatortable = denominator.style.format(\"{:g}\")\n",
    "cm = seaborn.light_palette(\"blue\", as_cmap=True)\n",
    "s = denominatortable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"matches_count.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of hotspot matches < 5000m\n",
    "numerator = pd.pivot_table(nearest_points[(nearest_points['dist_m'] < 5000)],values='count', index=['2_satellite_sensor_product'], columns=['satellite_sensor_product'], aggfunc={'count':len})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn styling for matrix\n",
    "numeratortable = numerator.style.format(\"{:g}\")\n",
    "cm = seaborn.light_palette(\"red\", as_cmap=True)\n",
    "s = numeratortable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"matches_5000m.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of matched points closer than 5000m\n",
    "difference = (denominator - numerator).style.format(\"{:g}\") \n",
    "# Set seaborn styling for matrix\n",
    "cm = seaborn.light_palette(\"red\", as_cmap=True)\n",
    "s = difference.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"count_difference.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of matched points closer than 5000m\n",
    "percentage = (numerator / denominator).style.format(\"{:.0%}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn styling for matrix\n",
    "cm = seaborn.light_palette(\"green\", as_cmap=True)\n",
    "s = percentage.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"percentage.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum time between matched points < 5000m\n",
    "timemax = pd.pivot_table(nearest_points[(nearest_points['dist_m'] < 5000)],values='timedelta', index=['satellite_sensor_product'], columns=['2_satellite_sensor_product'], aggfunc={'timedelta':np.max})\n",
    "# Set seaborn styling for matrix\n",
    "timemaxtable = timemax.style.format(\"{:}\")\n",
    "cm = seaborn.light_palette(\"gray\", as_cmap=True)\n",
    "s = timemaxtable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"max_time_matched_points.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render(annot=True))\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum time between matched points < 5000m\n",
    "timemin = pd.pivot_table(nearest_points[(nearest_points['dist_m'] < 5000)],values='timedelta', index=['2_satellite_sensor_product'], columns=['satellite_sensor_product'], aggfunc={'timedelta':np.min})\n",
    " \n",
    "# Set seaborn styling for matrix\n",
    "timemintable = timemin.style.format(\"{:}\")\n",
    "cm = seaborn.light_palette(\"purple\", as_cmap=True)\n",
    "s = timemintable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"min_time_matched_points.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average distance (m) between matched points < 5000m\n",
    "averagedist = pd.pivot_table(nearest_points[(nearest_points['dist_m'] < 5000)],values='dist_m', index=['2_satellite_sensor_product'], columns=['satellite_sensor_product'], aggfunc={'dist_m':np.mean})\n",
    "# Set seaborn styling for matrix\n",
    "averagedisttable = averagedist.style.format(\"{:}\")\n",
    "cm = seaborn.light_palette(\"olive\", as_cmap=True)\n",
    "s = averagedisttable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"avg_distance_5000m.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done - TODO polygon intersection (using satellite footprint tool and S3 geojson)\n",
    "# TODO match available Landsat and Sentinel 2 datasets with hotspots\n",
    "# TODO Use WCS to interact with GSKY for DEA data queries - https://gsky.readthedocs.io/en/latest/_notebook/Notebook_GSKY_WCS.html\n",
    "# TODO run fire detection on Landsat and Sentinel 2 - update attributes\n",
    "# Done - TODO rerun the nearest test with S3 as the primary, as well as DEA Hotspots.\n",
    "# Done - TODO add Himawari Hotspots from SRSS and WFABBA.(Done - see config.yaml, added password and request handling, concatenated GPD added)\n",
    "# Done - TODO add persistent hotspots and compare frequency of detection\n",
    "# TODO - get feedback on the length of day between midnight on the east side and midnight on the west side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions to answer\n",
    "# How to define coincidence?\n",
    "# How to constrain results - based on confidence? and minimum allowable radius?\n",
    "# Perhaps groupby time should be a moving window of two days or only look at matching hotspots first detected early in the day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Add moving window function i.e shift solar day by x hours forward or back and redo analysis\n",
    "# Done TODO - Add time delta for matches as well - average time delta might be a useful statistic\n",
    "# Done TODO - Add product group attribute to DEA dataframe\n",
    "# Done TODO roll this up to the nearest comparison to simplify product handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_points.to_csv('nearest_points.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - improve Results presentation\n",
    "# Plot time against count of matches\n",
    "# Plot vectors from one hotspot source to its match\n",
    "# TODO add vectors linking spots to their nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persistent Hotspot Comparison\n",
    "Landgate developed a set of persistent hotspots  (v5.2) by associating recurrent hotspots with known heat sources (industrial activity). Knowledge of persistent hotspots are used here to provide a baseline for the ability of a given sensor to detect hotspots. \n",
    "\n",
    "Persistent hotspots, together with high resolution hotspot sources from Sentinel 2 MSI and Landsat Enhanced Thematic Mapper and Opertional Land Imager sensors are used here to validate hotspots from the AVHRR, VIIRS, MODIS and SLSTR instruments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_hotspots = gpd.GeoDataFrame.from_file('zip://vectors/Known non FHS - Version 5.2.zip/Version 5.2/known_non_FHS.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO match DEA Hotspots to persistent hotspots\n",
    "# TODO match S3VT Hotspots to persistent hotspots\n",
    "# TODO how often do matched hotspots match with perisistent hotspots\n",
    "# TODO - improved logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent = ckdnearest( hotspots_gdf, persistent_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent['dist_m'] = nearest_persistent.apply(lambda row: distance((row.latitude, row.longitude),(row['2_Latitude'], row['2_Longitude'])).meters, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of hotspot matches \n",
    "persistentcount = pd.pivot_table(nearest_persistent,values='count', index=['2_Comment'], columns=['satellite_sensor_product'], aggfunc={'count':np.sum})\n",
    "#persistentcount\n",
    "# Set seaborn styling for matrix\n",
    "persistentcounttable = persistentcount.style.format(\"{:g}\")\n",
    "cm = seaborn.light_palette(\"orange\", as_cmap=True)\n",
    "s = persistentcounttable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"persistent_matches.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count of hotspot matches < 5000m\n",
    "persistentcountm = pd.pivot_table(nearest_persistent[(nearest_persistent['dist_m'] < 5000)],values='count', index=['2_Comment'], columns=['satellite_sensor_product'], aggfunc={'count':np.sum})\n",
    "# Set seaborn styling for matrix\n",
    "persistentcountmtable = persistentcountm.style.format(\"{:g}\")\n",
    "cm = seaborn.light_palette(\"blue\", as_cmap=True)\n",
    "s = persistentcountmtable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"peristent_matches_5000m.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent['line_geometry'] = nearest_persistent.apply(lambda row: shapely.wkt.loads(LineString([row.geometry, row['2_geometry']]).wkt), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent = nearest_persistent.set_geometry('line_geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent.geometry.plot(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gjson = persistent_hotspots.geometry.to_crs(epsg='4326').to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa = folium.Map([-26, 132],\n",
    "                  zoom_start=4,\n",
    "                  tiles='Stamen Terrain')\n",
    "\n",
    "points = folium.features.GeoJson(gjson)\n",
    "\n",
    "mapa.add_child(points)\n",
    "mapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Burnt Area Mapping Comparison \n",
    "Burnt area mapping comparisons to NSW Rural Fire Service isochrons drawn from aerial thermal line scanner and other high resolution sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isochrons - burn history from RFS\n",
    "rfs_isochrons = gpd.GeoDataFrame.from_file('zip://vectors/NSW_Isochrons_Progall_ffdi_v3.zip/progall_ffdi_v3.shp')\n",
    "rfs_isochrons.crs = 'epsg:3308'\n",
    "rfs_isochrons.to_crs('epsg:4326', inplace=True)\n",
    "rfs_isochrons['time'] = pd.to_datetime(rfs_isochrons.time)\n",
    "rfs_isochrons['time'] = rfs_isochrons['time'].dt.tz_localize(tz='Australia/Sydney')\n",
    "rfs_isochrons['lasttim'] = pd.to_datetime(rfs_isochrons.lasttim)\n",
    "rfs_isochrons['lasttim'] = rfs_isochrons['lasttim'].dt.tz_localize(tz='Australia/Sydney')\n",
    "rfs_isochrons['solar_day'] = rfs_isochrons.apply(lambda row: solar_day(row.time, 150), axis = 1)\n",
    "rfs_isochrons = rfs_isochrons.set_index(pd.DatetimeIndex(rfs_isochrons.solar_day.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the area of interest\n",
    "westlon, southlat, eastlon, northlat = rfs_isochrons.total_bounds\n",
    "bbox = (westlon, southlat, eastlon, northlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isochrons = rfs_isochrons.geometry.to_json()\n",
    "hotspots = hotspots_gdf.loc['2019-11-02':'2019-11-02'].geometry.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa = folium.Map([-26, 132],\n",
    "                  zoom_start=4,\n",
    "                  tiles='Stamen Terrain')\n",
    "\n",
    "points = folium.features.GeoJson(isochrons)\n",
    "mapa.add_child(points)\n",
    "points = folium.features.GeoJson(hotspots)\n",
    "mapa.add_child(points)\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection_list = []\n",
    "for Aname, Agroup in rfs_isochrons.resample('D', on='solar_day'):\n",
    "    for Bname, Bgroup in hotspots_gdf.resample('D', on='solar_day'):\n",
    "        if Aname == Bname:\n",
    "            try:\n",
    "                Agroup = Agroup.reset_index()\n",
    "                Bgroup = Bgroup.reset_index()\n",
    "                intersection = gpd.sjoin(Agroup, Bgroup, op='intersects')\n",
    "                print(Aname, 'intersection success')\n",
    "                intersection_list.append(intersection)\n",
    "            except:\n",
    "                print(Aname, 'intersection error')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isochron_intersect = gpd.GeoDataFrame(pd.concat(intersection_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isochron_intersect.reset_index()\n",
    "isochron_intersect['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isochron_intersect['solar_day'] = isochron_intersect.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isochron_intersect['solar_date'] = isochron_intersect.solar_day_right.dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.pivot_table(isochron_intersect, values='count', index=['solar_date'], columns=['satellite_sensor_product'], aggfunc={'count':np.sum})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
