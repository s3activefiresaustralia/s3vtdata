{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from netCDF4 import Dataset\n",
    "import datetime as dt\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "import shapely.speedups\n",
    "shapely.speedups.enable()\n",
    "import xmltodict\n",
    "import yaml\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.geometry import Point, LineString\n",
    "from geopy.distance import distance\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(level=logging.INFO)\n",
    "logging.basicConfig(filename='notebook.log',level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open configuration file and read parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'config.yaml') as file:\n",
    "    configuration =  yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in configuration['configurations']:\n",
    "    username = config['username']\n",
    "    password = config['password']\n",
    "    url = config['url']\n",
    "    aoi = config['aoi']\n",
    "    awss3bucket = config['awss3bucket']\n",
    "    awskeyid = config['awskeyid']\n",
    "    awskeypass = config['awskeypass']\n",
    "    hotspotslogin = config['hotspots_login']\n",
    "    hotspotspassword = config['hotspots_password']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_satellite_swaths(configuration, start, period, solar_day):\n",
    "    \"\"\"\n",
    "    Function to determine the common imaging footprint of a pair of sensors\n",
    "    \n",
    "    Returns the imaging footprints of a pair of sensors for a period from a starting datetime\n",
    "    \"\"\"\n",
    "    output = Path('output')\n",
    "    dirpath = Path.joinpath(output, solar_day)\n",
    "    \n",
    "    if (dirpath.exists()):\n",
    "        logger.info(str(solar_day)+\" exists - skipping swath generation\")\n",
    "        success = True\n",
    "    else:\n",
    "        dirpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    \n",
    "        try:\n",
    "            logger.info('Generating swaths '+str(['python', 'swathpredict.py', '--configuration', configuration, '--start', start, '--period', period, '--output_path', str(dirpath)])\n",
    ")\n",
    "            subprocess.call(['python', 'swathpredict.py', '--configuration', configuration, '--start', start, '--period', period, '--output_path', str(dirpath)])\n",
    "            \n",
    "            success = True\n",
    "        except:\n",
    "            success = False\n",
    "            logger.info('Swath generation failed')\n",
    "    \n",
    "    return(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_swath_intersect(satsensorsA, satsensorsB, solar_day):\n",
    "    logging.info(\"Running intersection for \"+str(satsensorsA)+' '+str(satsensorsB))\n",
    "    satsensorsA = [w.replace(' ', '_') for w in satsensorsA]\n",
    "    satsensorsB = [w.replace(' ', '_') for w in satsensorsB]\n",
    "        \n",
    "    filesA = []\n",
    "    filesB = []\n",
    "    \n",
    "    output = Path('output')\n",
    "    dirpath = Path.joinpath(output, solar_day)\n",
    "    \n",
    "    for sat in satsensorsA:\n",
    "   \n",
    "        filesA.extend([f for f in os.listdir(str(dirpath)) if (sat in f )and ('swath.geojson' in f)])\n",
    "        \n",
    "    for sat in satsensorsB:\n",
    "    \n",
    "        filesB.extend([f for f in os.listdir(str(dirpath)) if sat in f and 'swath.geojson' in f])\n",
    "    \n",
    "    gpdlistA = []\n",
    "    for file in filesA:\n",
    "        df = gpd.read_file(Path.joinpath(dirpath, file))\n",
    "        gpdlistA.append(df)\n",
    "    gpdlistB = []\n",
    "    for file in filesB:\n",
    "        df = gpd.read_file(Path.joinpath(dirpath, file))\n",
    "        gpdlistB.append(df)        \n",
    "    return(pd.concat(gpdlistA),pd.concat(gpdlistB) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_list(username, password, aoi, startrecord):\n",
    "    # rows returned is limited to 100, add pagination but looking at number of records and incrementing by 100 each iteration\n",
    "    \"\"\"\n",
    "    Function to determine available files for download from S3 expert hub via wget from a start record\n",
    "    \n",
    "    Returns success if the wget function has executed \n",
    "    \"\"\"\n",
    "    try:\n",
    "        subprocess.call(['wget','--no-check-certificate', '--user='+username, '--password='+password, '--output-document=filelist.txt', 'https://131.176.236.38/dhus/search?q=footprint:\"Intersects('+aoi+')\" AND platformname:Sentinel-3 AND producttype:SL_2_FRP___&rows=100&start='+startrecord+'&format=json'])\n",
    "        success = True\n",
    "    except:\n",
    "        success = False\n",
    "    return(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(sensors):\n",
    "    \"\"\"\n",
    "    Function to construct a filter statement for input to geoserver WFS query\n",
    "    \n",
    "    Returns string for input to geoserver WFS query\n",
    "    \"\"\"\n",
    "    for sensordict in sensors:\n",
    "        \n",
    "        filter_string = ''\n",
    "        count = 0\n",
    "        \n",
    "        for sensor in sensordict.keys():\n",
    "            filter_string = filter_string+'(sensor=%27'+sensor+'%27%20AND%20(product=%27'\n",
    "            product_count = 0\n",
    "            for product in sensordict[sensor]:\n",
    "                filter_string = filter_string+product+'%27'\n",
    "                if product_count < (len(sensordict[sensor])-1):\n",
    "                    filter_string = filter_string+'%20OR%20product=%27'\n",
    "                else:\n",
    "                    filter_string = filter_string+'))' \n",
    "                product_count = product_count + 1\n",
    "            if count < (len(sensordict.keys())-1):        \n",
    "                filter_string = filter_string+'%20OR%20'\n",
    "            count = count+1\n",
    "\n",
    "    return(filter_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polygon_from_gml(gml_dict):\n",
    "    \"\"\"\n",
    "    Function to construct a polygon from the GML within the S3 Expert hub wget query response\n",
    "    \n",
    "    Returns a list of tuples representing the polygon for the imaging extent\n",
    "    \"\"\"\n",
    "    listoftuples = []\n",
    "    for i in list(gml_dict.split(\" \")):\n",
    "        pair = (float(i.split(',')[1]), float(i.split(',')[0]))\n",
    "        listoftuples.append(pair)\n",
    "    return(listoftuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hotspots(filter_string, time_period, bbox, max_features, min_confidence, to_date, login, password):\n",
    "    \"\"\"\n",
    "    Function to find available DEA Hotspots from a WFS query within bounding box for given time range\n",
    "    and set of sensor\n",
    "    \n",
    "    Returns a geopandas dataframe for the DEA Hotspot points matching the query criteria\n",
    "    \"\"\"\n",
    "    y_max = bbox[0]\n",
    "    x_min = bbox[1]\n",
    "    y_min = bbox[2]\n",
    "    x_max = bbox[3]\n",
    "    if to_date is None:\n",
    "        \n",
    "        to_date = dt.datetime.now()\n",
    "    \n",
    "    \n",
    "    from_date = (to_date - dt.timedelta(days=time_period)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # trim datetime to enable WFS \n",
    "    to_date = to_date.strftime('%Y-%m-%d')\n",
    "    logger.info(str(from_date)+' '+str(to_date))\n",
    "    # TODO - sort out paging - looks like there is a limit to WFS requests number returned per query\n",
    "    # First query - count how many records in response - if = add to start record in query from previous loop\n",
    "    \n",
    "    start_index = 0\n",
    "    #feature_count = max_features\n",
    "    gpd_list = []\n",
    "    logger.info(f\"https://{login}:{password}@hotspots.dea.ga.gov.au/geoserver/public/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=public:hotspots&outputFormat=application/json&CQL_FILTER=({filter_string})%20AND%20datetime%20%3E%20%27{from_date}%27%20AND%20datetime%20%3C%20%27{to_date}%27%20AND%20INTERSECTS(location,%20POLYGON(({y_max}%20{x_min},%20{y_max}%20{x_max},%20{y_min}%20{x_max},%20{y_min}%20{x_min},%20{y_max}%20{x_min})))&maxFeatures=1&startIndex=0&sortBy=sensor%20A\")\n",
    "    url = f\"https://hotspots.dea.ga.gov.au/geoserver/public/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=public:hotspots&outputFormat=application/json&CQL_FILTER=({filter_string})%20AND%20datetime%20%3E%20%27{from_date}%27%20AND%20datetime%20%3C%20%27{to_date}%27%20AND%20INTERSECTS(location,%20POLYGON(({y_max}%20{x_min},%20{y_max}%20{x_max},%20{y_min}%20{x_max},%20{y_min}%20{x_min},%20{y_max}%20{x_min})))&maxFeatures=1&startIndex=0&sortBy=sensor%20A\"\n",
    "    \n",
    "    try:\n",
    "        data = requests.get(url, auth=HTTPBasicAuth(login, password))\n",
    "    except:\n",
    "        logger.info(\"URL request rejected\")\n",
    "        \n",
    "    totalfeatures = data.json()['totalFeatures']\n",
    "    \n",
    "    while start_index <= totalfeatures:\n",
    "        logger.info(f\"https://{login}:{password}@hotspots.dea.ga.gov.au/geoserver/public/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=public:hotspots&outputFormat=application/json&CQL_FILTER=({filter_string})%20AND%20datetime%20%3E%20%27{from_date}%27%20AND%20datetime%20%3C%20%27{to_date}%27%20AND%20INTERSECTS(location,%20POLYGON(({y_max}%20{x_min},%20{y_max}%20{x_max},%20{y_min}%20{x_max},%20{y_min}%20{x_min},%20{y_max}%20{x_min})))&maxFeatures={max_features}&startIndex={start_index}&sortBy=sensor%20A\")\n",
    "        url = f\"https://hotspots.dea.ga.gov.au/geoserver/public/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=public:hotspots&outputFormat=application/json&CQL_FILTER=({filter_string})%20AND%20datetime%20%3E%20%27{from_date}%27%20AND%20datetime%20%3C%20%27{to_date}%27%20AND%20INTERSECTS(location,%20POLYGON(({y_max}%20{x_min},%20{y_max}%20{x_max},%20{y_min}%20{x_max},%20{y_min}%20{x_min},%20{y_max}%20{x_min})))&maxFeatures={max_features}&startIndex={start_index}&sortBy=sensor%20A\"\n",
    "    \n",
    "        try:\n",
    "            data = requests.get(url, auth=HTTPBasicAuth(login, password))\n",
    "        except:\n",
    "            logger.info(\"URL request rejected\")\n",
    "        \n",
    "        gpd_list.append(gpd.read_file(json.dumps(data.json())))\n",
    "        \n",
    "        start_index = start_index + max_features\n",
    "        \n",
    "    hotspots_gdf = pd.concat(gpd_list)\n",
    "    \n",
    "    # Reset the index because you just concatenated dataframes\n",
    "    hotspots_gdf = hotspots_gdf.reset_index()\n",
    "    \n",
    "    # TODO - improved None value handling  -currently just look at first and apply that to all\n",
    "    if hotspots_gdf['confidence'][0] == None:\n",
    "        logger.info('Skipping confidence filter as confidence not populated')\n",
    "    else:\n",
    "        # Filter by confidence\n",
    "        hotspots_gdf = hotspots_gdf.loc[hotspots_gdf.confidence >= min_confidence]\n",
    "\n",
    "    # Fix datetime\n",
    "    if hotspots_gdf['start_dt'][0] == None:\n",
    "        logger.info('Start date field is not populated')\n",
    "        hotspots_gdf['datetime'] = pd.to_datetime(hotspots_gdf['datetime'])\n",
    "    else:\n",
    "        hotspots_gdf['datetime'] = pd.to_datetime(hotspots_gdf['start_dt'])\n",
    "\n",
    "    # Extract required columns\n",
    "    hotspots_gdf = hotspots_gdf.loc[:, [\n",
    "            'datetime', 'latitude', 'longitude', 'confidence', 'geometry', 'product', 'satellite', 'sensor', 'power'\n",
    "            ]]\n",
    "    hotspots_gdf.sort_values('datetime', ascending=True, inplace=True)\n",
    "    logger.info('Hotspots loaded successfully '+str(hotspots_gdf.geometry.total_bounds))\n",
    "\n",
    "    return(hotspots_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar_day_start_stop_period(longitude_east, longitude_west, solar_day):\n",
    "    \"\"\"\n",
    "    Function solar day start time from longitude and solar day in utc \n",
    "    \n",
    "    Returns datetime start stop in utc and period between in minutes\n",
    "    \"\"\"\n",
    "    # Solar day time relative to UTC and local longitude\n",
    "    SECONDS_PER_DEGREE = 240\n",
    "    # Offset for eastern limb\n",
    "    offset_seconds_east = int(longitude_east * SECONDS_PER_DEGREE)\n",
    "    offset_seconds_east = np.timedelta64(offset_seconds_east, 's')\n",
    "    # offset for wester limb\n",
    "    offset_seconds_west = int(longitude_west * SECONDS_PER_DEGREE)\n",
    "    offset_seconds_west = np.timedelta64(offset_seconds_west, 's')\n",
    "    # time between two limbs\n",
    "    offset_day = np.timedelta64(1440, 'm') + abs(offset_seconds_east - offset_seconds_west)\n",
    "    #ten_am_crossing_adjustment = np.timedelta64(120, 'm')\n",
    "    # Solar day start at eastern limb\n",
    "    solar_day_start_utc = (np.datetime64(solar_day) - offset_seconds_east ).astype(datetime)\n",
    "    # Solar day finish at western limb\n",
    "    solar_day_finish_utc = ((np.datetime64(solar_day)+offset_day)  - offset_seconds_east ).astype(datetime)\n",
    "    # Duration of solar day\n",
    "    solar_day_duration = np.timedelta64((solar_day_finish_utc - solar_day_start_utc), 'm' )\n",
    "    \n",
    "    return(solar_day_start_utc, solar_day_finish_utc , solar_day_duration.astype(datetime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startutc, endutc, duration = solar_day_start_stop_period(150, 110, datetime(2020, 5, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startutc, endutc, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar_day(utc, longitude):\n",
    "    \"\"\"\n",
    "    Function solar day for a given UTC time and longitude input\n",
    "    \n",
    "    Returns datetime object representing solar day\n",
    "    \"\"\"\n",
    "    SECONDS_PER_DEGREE = 240\n",
    "    offset_seconds = int(longitude * SECONDS_PER_DEGREE)\n",
    "    offset = np.timedelta64(offset_seconds, 's')\n",
    "    return (np.datetime64(utc) + offset).astype(datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_day(startutc, 110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ckdnearest(gdA, gdB):\n",
    "    \"\"\"\n",
    "    Function to find points in \"B\" nearest to \"A\" geopandas dataframe\n",
    "    \n",
    "    Returns geopandas dataframe with records representing matches\n",
    "    \"\"\"\n",
    "    nA = np.array(list(zip(gdA.geometry.x, gdA.geometry.y)) )\n",
    "    nB = np.array(list(zip(gdB.geometry.x, gdB.geometry.y)) )\n",
    "    btree = cKDTree(nB)\n",
    "    dist, idx = btree.query(nA, k=1)\n",
    "    gdf = pd.concat(\n",
    "        [gdA.reset_index(drop=True), gdB.loc[idx].reset_index(drop=True).add_prefix('2_'),\n",
    "         pd.Series(dist, name='dist')], axis=1)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the area of interest\n",
    "westlon = 110.0\n",
    "southlat = -50.0\n",
    "eastlon = 160.0\n",
    "northlat = -10.0\n",
    "bbox = (westlon, southlat, eastlon, northlat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip to \"Load Hotspots to GeoPandas\" if testing analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess inventory against AWS bucket listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bucket listing of image granules and hotspot files\n",
    "\n",
    "s3 = boto3.resource('s3', aws_access_key_id=awskeyid,\n",
    "                    aws_secret_access_key=awskeypass)\n",
    "\n",
    "s3folderlist = []\n",
    "s3geojsonlist = []\n",
    "s3bucket = s3.Bucket('s3vtaustralia')\n",
    "\n",
    "for bucket_object in s3bucket.objects.all():\n",
    "    s3bucketobject = str(bucket_object.key).split(\"/\")[2]\n",
    "    if '.SEN3' in s3bucketobject:\n",
    "        s3folderlist.append(s3bucketobject)\n",
    "    if '.FRP.geojson' in s3bucketobject:\n",
    "        s3geojsonlist.append(bucket_object.key)\n",
    "\n",
    "print(len(s3geojsonlist), \"S3 Hotspot files From ESA available in AWS S3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(s3folderlist)), \"S3 Granules From ESA available in AWS S3\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the file list inventory used to gather the S3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get File List from server\n",
    "# Run this if no local inventory exists\n",
    "if not os.path.exists('s3vt_inventory.json'):\n",
    "    startrecord = 0\n",
    "\n",
    "    responselist = [] \n",
    "\n",
    "    # Determine number of records to retrieve\n",
    "    get_file_list(username, password, aoi, str(startrecord))\n",
    "    with open('filelist.txt') as results:\n",
    "        for i in results: \n",
    "            response = json.loads(i)\n",
    "            responselist.append(response)\n",
    "\n",
    "    upperlimit = int(response['feed']['opensearch:totalResults'])\n",
    "    upperlimit = 200\n",
    "    # Get the full list of records\n",
    "\n",
    "    while startrecord <= upperlimit:\n",
    "        startrecord = startrecord+100\n",
    "\n",
    "        get_file_list(username, password, aoi, str(startrecord)) \n",
    "        with open('filelist.txt') as results:\n",
    "            for i in results: responselist.append(json.loads(i))\n",
    "\n",
    "            # Dump the results to an inventory file\n",
    "            with open('s3vt_inventory.json', 'w') as f:\n",
    "                json.dump(responselist, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From the inventory - generate a vector fooprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read inventory to geopandas - write to geojson       \n",
    "        \n",
    "with open('s3vt_inventory.json') as inventory:\n",
    "    frames = []\n",
    "    for p in inventory:\n",
    "        pages = json.loads(p)\n",
    "                \n",
    "        for page in pages:\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    \n",
    "                    for entry in page['feed']['entry']:\n",
    "\n",
    "                        df = pd.DataFrame.from_dict(entry, orient='index')\n",
    "\n",
    "                        polygon = get_polygon_from_gml(xmltodict.parse(entry['str'][2]['content'])['gml:Polygon']['gml:outerBoundaryIs']['gml:LinearRing']['gml:coordinates'])\n",
    "\n",
    "                        df = df.transpose()\n",
    "                        df['Coordinates'] = Polygon(polygon)\n",
    "                        for d in entry['str']:\n",
    "                            if d['name'] ==  'orbitdirection':\n",
    "                                df['orbitdirection'] = d['content']\n",
    "                            if d['name'] ==  'platformidentifier':\n",
    "                                df['platformidentifier'] = d['content'] \n",
    "                            if d['name'] ==  'filename':\n",
    "                                df['filename'] = d['content']\n",
    "                            if d['name'] ==  'instrumentshortname':\n",
    "                                df['instrumentshortname'] = d['content']\n",
    "                            if d['name'] ==  'passnumber':\n",
    "                                df['passnumber'] = d['content']        \n",
    "                        s3vtdf = gpd.GeoDataFrame(df, geometry='Coordinates')\n",
    "\n",
    "                        frames.append(s3vtdf)\n",
    "                                \n",
    "                except KeyError:\n",
    "                        logger.info(\"KeyError exception for get_polygon_from_gml()\")\n",
    "                        \n",
    "s3vtgpd = pd.concat(frames)\n",
    "\n",
    "# Not sure why we need to index but do it anyway\n",
    "s3vtgpd = s3vtgpd.reset_index(drop=True)\n",
    "s3vtgpd['date'] = pd.to_datetime(s3vtgpd.summary.str.split(\",\", expand= True)[0].str.split(' ', expand=True)[1])\n",
    "# Some fields are lists and geojson translation doesn't like it\n",
    "\n",
    "s3vtgpd = s3vtgpd.drop(['link', 'int', 'str', 'summary'], axis=1)\n",
    "s3vtgpd.to_file('s3vt_geometry.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up for checking inventory against files on AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframelength = len(s3vtgpd)\n",
    "# Add field to enable monitoring\n",
    "s3vtgpd['hotspot'] = 0\n",
    "s3vtgpd['download'] = 0\n",
    "s3vtgpd['s3bucket'] = 0\n",
    "\n",
    "s3vthostpotsgpdlist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run inventory check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if folder already downloaded and flag in gpd\n",
    "for i in range(dataframelength):\n",
    "    if s3vtgpd.loc[i]['title']+'.SEN3' in set(s3folderlist):\n",
    "        s3vtgpd.at[i, 'download'] = 1\n",
    "    if s3vtgpd.loc[i]['title']+'.FRP.geojson' in set(s3folderlist):\n",
    "        s3vtgpd.at[i, 'hotspot'] = 1\n",
    "        s3vthostpotsgpdlist.append(s3hotspotsgpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync GeoJSON to local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all S3 geojson files locally and load to GPD\n",
    "# TODO - fix the below subprocess as it doesn't seem to accept the exclude include parameters and syncs everything\n",
    "#subprocess.call(['echo', 'aws', 's3', 'sync', 's3://s3vtaustralia/', '.', '--exclude', '\\\"*\\\"', '--include', '\\\"*.geojson\\\"', '--dryrun'])\n",
    "# Going with this in the interim\n",
    "!aws s3 sync s3://s3vtaustralia/ . --exclude \"*\" --include \"*.geojson\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter hotspots based on limitations stated by UCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for i in s3geojsonlist:\n",
    "    df1 = gpd.read_file(i, bbox=bbox)\n",
    "    if 'S3A' in i:\n",
    "        df1['satellite'] = 'SENTINEL_3A'\n",
    "    else:\n",
    "        df1['satellite'] = 'SENTINEL_3B'\n",
    "    df1['sensor'] = 'SLSTR'\n",
    "    df2 = df1.query(\"FRP_MWIR>0\")\n",
    "    if len(df2) > 0:\n",
    "        frames.append(df2)\n",
    "        \n",
    "s3vthotspots = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = None\n",
    "s3geojsonlist = None\n",
    "df = None\n",
    "df1 = None\n",
    "df2 = None\n",
    "s3vtgpd = None\n",
    "s3vthostpotsgpdlist = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert netcdf CF time to something pandas understands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots['date'] = pd.to_datetime(netCDF4.num2date(s3vthotspots.time, units='microseconds since 2000-01-01T00:00:00Z', only_use_cftime_datetimes=False, only_use_python_datetimes=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add solar day column to enable group by function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots['solar_day'] = s3vthotspots.apply(lambda row: solar_day(row.date, row.longitude), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write S3 Hotspots to GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots.to_file('s3vt_hotspots.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = list(s3vthotspots.geometry.total_bounds)\n",
    "wfsbbox = [bounds[3], bounds[0], bounds[1], bounds[2]]\n",
    "wfsbbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3vthotspots = gpd.read_file('s3vt_hotspots.geojson', bbox=bbox)\n",
    "#s3vthotspots['datetime'] = pd.to_datetime(s3vthotspots['date'])\n",
    "#s3vthotspots['solar_day'] = pd.to_datetime(s3vthotspots['solar_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get time bounds for DEA Hotspots query\n",
    "\n",
    "maxdate = s3vthotspots.date.max().to_datetime64()\n",
    "mindate = s3vthotspots.date.min().to_datetime64()\n",
    "to_date = dt.datetime.strptime(str(s3vthotspots.date.max().to_datetime64()), '%Y-%m-%dT%H:%M:%S.%f000') # '2018-01-01T00:00:00.000Z'\n",
    "time_period = int(str(np.timedelta64(maxdate - mindate, 'D')).split(' ')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing DEA Hotspots load\n",
    "#hotspots_gdf = load_hotspots('(sensor=%27AVHRR%27%20AND%20(product=%27SRSS%27%20OR%20product=%27GA%27))%20OR%20(sensor=%27MODIS%27%20AND%20(product=%27MOD14%27%20OR%20product=%27SRSS%27))%20OR%20(sensor=%27VIIRS%27%20AND%20(product=%27AFMOD%27%20OR%20product=%27AFIMG%27%20OR%20product=%27EDR%27%20OR%20product=%27SRSS%27))',88, [12.234104969111854, -179.98885754557182, -46.4811018826465, 179.9494145456714], 300000, 0 , dt.datetime.strptime('2020-05-01 02:23:39.493931', '%Y-%m-%d %H:%M:%S.%f'), 'hotspots', 'F1r3f1ght3R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in configuration['configurations']:\n",
    "    \n",
    "    hotspots_gdf = load_hotspots(filter(config['sensors']),\n",
    "                                         time_period,\n",
    "                                         wfsbbox,\n",
    "                                         config['max_features'], \n",
    "                                         config['min_confidence'],\n",
    "                                         to_date,\n",
    "                                         config['hotspots_login'],\n",
    "                                         config['hotspots_password'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf['solar_day'] = hotspots_gdf.apply(lambda row: solar_day(row.datetime, row.longitude), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.remove('DEAHotspots_hotspots.geojson')\n",
    "except:\n",
    "    logger.info('DEA Hotspots geojson does not exist or cannot be deleted')\n",
    "    \n",
    "hotspots_gdf.to_file('DEAHotspots_hotspots.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Hotspots to GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf = gpd.read_file('DEAHotspots_hotspots.geojson', bbox=bbox)\n",
    "hotspots_gdf['datetime'] = pd.to_datetime(hotspots_gdf['datetime'])\n",
    "hotspots_gdf['solar_day'] = pd.to_datetime(hotspots_gdf['solar_day'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots = gpd.read_file('s3vt_hotspots.geojson', bbox=bbox)\n",
    "s3vthotspots['datetime'] = pd.to_datetime(s3vthotspots['date'])\n",
    "s3vthotspots['solar_day'] = pd.to_datetime(s3vthotspots['solar_day'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up S3 Hotspots to allow single GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots.rename(columns={'F1_Fire_pixel_radiance':'power'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots = s3vthotspots.drop(['FRP_MWIR', 'FRP_SWIR', 'FRP_uncertainty_MWIR',\n",
    "       'FRP_uncertainty_SWIR', 'Glint_angle', 'IFOV_area', 'Radiance_window',\n",
    "       'S7_Fire_pixel_radiance', 'TCWV', 'classification',  'i',\n",
    "       'j', 'n_SWIR_fire', 'n_cloud', 'n_water',\n",
    "       'n_window', 'time', 'transmittance_MWIR', 'transmittance_SWIR',\n",
    "       'used_channel', 'date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots['satellite_sensor_product'] = s3vthotspots['satellite']+'_'+s3vthotspots['sensor']+'_ESA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf['satellite_sensor_product'] = hotspots_gdf['satellite']+'_'+hotspots_gdf['sensor']+'_'+hotspots_gdf['product']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf = hotspots_gdf.drop(['product'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf = pd.concat([hotspots_gdf, s3vthotspots])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty the S3 geodataframe object\n",
    "s3vthotspots = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating doesn't update the index automatically\n",
    "hotspots_gdf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map plot hotspots\n",
    "hotspots_gdf.plot(column='satellite_sensor_product', legend=True, legend_kwds={'loc': 'upper right'}, figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index by solar day to enable groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf =  hotspots_gdf.set_index(pd.DatetimeIndex(hotspots_gdf.solar_day.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf['satellite_sensor_product'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal subset (by solar day) to enable rapid testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2019-11-01'\n",
    "end_date = '2020-05-01'\n",
    "comparison_prefix = '20191101_20200501'\n",
    "hotspots_gdf = hotspots_gdf.loc[start_date:end_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf.loc[start_date:end_date]['datetime'].min(), hotspots_gdf.loc[start_date:end_date]['datetime'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf['satellite_sensor_product'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run comparison matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison sets\n",
    "setA = set(hotspots_gdf['satellite_sensor_product'])\n",
    "setAlist = []\n",
    "[setAlist.append(i) for i in setA]\n",
    "setAlist.sort()\n",
    "setAlist[7:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_prefix = '-'.join(setAlist[7:9])+'-'+comparison_prefix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all hotspot sources to each other\n",
    "\n",
    "appended_dataframe = []\n",
    "satellite_sensor_product_intersections = {}\n",
    "\n",
    "#for productA in set(hotspots_gdf['satellite_sensor_product']):\n",
    "for productA in setAlist[7:9]:\n",
    "    for productB in set(hotspots_gdf['satellite_sensor_product']):\n",
    "        \n",
    "        gdfA = hotspots_gdf[(hotspots_gdf['satellite_sensor_product'] == productA)]       \n",
    "        gdfB = hotspots_gdf[(hotspots_gdf['satellite_sensor_product'] == productB)]       \n",
    "\n",
    "        # For each solar day group in gdfA\n",
    "        for Aname, Agroup in gdfA.resample('D', on='solar_day'):\n",
    "            \n",
    "            minutctime, maxutctime, deltautctime = solar_day_start_stop_period(eastlon, westlon, Aname)\n",
    "     \n",
    "            # For each solar day group in gdfB\n",
    "            for Bname, Bgroup in gdfB.resample('D', on='solar_day'):      \n",
    "\n",
    "                # Do where the solar days are the same in gdfA and B\n",
    "                if (Aname == Bname):\n",
    "                    logger.info(productA+' '+productB)\n",
    "\n",
    "                    satellite_sensor_product_intersections['solar_day'] = Aname\n",
    "                    \n",
    "                    logger.info(str(Aname)+' '+str(minutctime)+' '+str(minutctime)+' '+str(deltautctime))\n",
    "                    \n",
    "                    # Generate the GeoJSON for each satellite in s3vtconfig.yaml\n",
    "                    \n",
    "                    get_satellite_swaths('s3vtconfig.yaml', minutctime.strftime(\"%Y-%m-%dT%H:%M:%SZ\"), str(int(deltautctime.total_seconds()/60)), str(Aname.date()))\n",
    "                    \n",
    "                    # Geostationary satellites need an exception\n",
    "                    if not (('AHI' in [productA, productB]) or ('INS1' in [productA, productB])):\n",
    "\n",
    "                        # Include a try except to counteract failures where swath intersect fails\n",
    "                        try:\n",
    "                            \n",
    "                            # Get geometries for satellite sensors in gpdA and gpdB\n",
    "                            gpd1, gpd2 = pairwise_swath_intersect(set(Agroup['satellite']), set(Bgroup['satellite']), str(Aname.date()))\n",
    "                            \n",
    "                            # TODO - limit the swath geometries used for intersection\n",
    "\n",
    "                            # Union before intersect\n",
    "                            gpd1 = gpd1.unary_union\n",
    "                            gpd2 = gpd2.unary_union\n",
    "                            \n",
    "                            # Intersect geometries\n",
    "                            intersection = gpd1.intersection(gpd2)\n",
    "                            logger.info(str(intersection))\n",
    "  \n",
    "                            \n",
    "                            if intersection == None:\n",
    "                                logger.info(\"Intersection is None\")\n",
    "                            else:\n",
    "                                logger.info(\"Intersection successful\")\n",
    "                            # Use intersection results to subset points (compare common imaged area)\n",
    "                            \n",
    "                            logger.info(\"Before intersection \"+str(Aname)+' '+str(Agroup['satellite_sensor_product'].count())+' '+str(Bgroup['satellite_sensor_product'].count()))\n",
    "                            \n",
    "                            pip_mask = Agroup.within(intersection)\n",
    "                            Agroup = Agroup.loc[pip_mask]                                \n",
    "                            Agroup.reset_index(drop=True, inplace=True)\n",
    "                            \n",
    "                            pip_mask = Bgroup.within(intersection)\n",
    "                            Bgroup = Bgroup.loc[pip_mask]\n",
    "                            Bgroup.reset_index(drop=True, inplace=True)\n",
    "                            logger.info(\"After intersection \"+str(Aname)+' '+str(Agroup['satellite_sensor_product'].count())+' '+str(Bgroup['satellite_sensor_product'].count()))\n",
    "                            \n",
    "                            if (Agroup['solar_day'].count() == 0) or (Bgroup['solar_day'].count() == 0):\n",
    "                                logger.info(\"Nothing to input to ckdnearest\")\n",
    "\n",
    "                            per_solarday_nearest_hotspots = ckdnearest(Agroup , Bgroup)\n",
    "                            \n",
    "                            #print(\"Matched \",per_solarday_nearest_hotspots['solar_day'].count(), \" to \", per_solarday_nearest_hotspots['2_geometry'].count())\n",
    "                            \n",
    "                            appended_dataframe.append(per_solarday_nearest_hotspots)\n",
    "                            #print(len(appended_dataframe))\n",
    "                            \n",
    "                        except:\n",
    "                            logger.info('Skipping')\n",
    "                    else:\n",
    "                        # Himawari AHI or INS1 geostationary case\n",
    "                        # A better approach here is to check if either has a swath available\n",
    "                        # If not - defer to the intersection of the one with a geometry\n",
    "                        # TODO - improve for Himawari\n",
    "                        try:\n",
    "                            per_solarday_nearest_hotspots = ckdnearest(Agroup.reset_index(drop=True, inplace=True), Bgroup.reset_index(drop=True, inplace=True))\n",
    "                            print(len(appended_dataframe))\n",
    "                            appended_dataframe.append(per_solarday_nearest_hotspots)\n",
    "                        except:\n",
    "                            logger.info('Skipping')\n",
    "\n",
    "nearest_points = pd.concat(appended_dataframe)\n",
    "appended_dataframe = None\n",
    "\n",
    "# Add metres distance between two points\n",
    "nearest_points['dist_m'] = nearest_points.apply(lambda row: distance((row.latitude, row.longitude),(row['2_latitude'], row['2_longitude'])).meters, axis = 1)\n",
    "# Add time delta between points\n",
    "nearest_points['timedelta'] = (abs(nearest_points['datetime'] - nearest_points['2_datetime']))\n",
    "nearest_points['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_points.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+'nearest_points.geojson'\n",
    "nearest_points.to_file(output, driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_points.set_index('solar_day', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "australia = gpd.GeoDataFrame.from_file('vectors/australia.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUGGING For testing intersect results for swath and hotspots\n",
    "\n",
    "satelliteA = 'SENTINEL_3B' # NOAA 19, NOAA 20, TERRA, AQUA, SENTINEL_3A, SENTINEL_3B\n",
    "satelliteB = 'NOAA 19'\n",
    "solar_date = '2020-01-04'\n",
    "gpd1, gpd2 = pairwise_swath_intersect([satelliteA], [satelliteB], solar_date)\n",
    "\n",
    "start_date = solar_date\n",
    "end_date = solar_date\n",
    "# uncomment below to examine data prior to intersect\n",
    "hotspots_sample = hotspots_gdf.loc[start_date:end_date]\n",
    "\n",
    "# uncomment below to examine data post intersect\n",
    "#nearest_sample = nearest_points[(nearest_points['satellite'] == satelliteA) & (nearest_points['2_satellite'] == satelliteB) ]\n",
    "#nearest_sample = pd.concat([nearest_sample, nearest_points[(nearest_points['satellite'] == satelliteB) & (nearest_points['2_satellite'] == satelliteA) ]])\n",
    "#hotspots_sample = nearest_sample.loc[start_date:end_date]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "gpd1.plot(ax=ax, facecolor='red');\n",
    "gpd2.plot(ax=ax, color='green');\n",
    "hotspots_sample[hotspots_sample['satellite'] == satelliteA].plot(ax=ax, color='tomato', markersize=2)\n",
    "hotspots_sample[hotspots_sample['satellite'] == satelliteB].plot(ax=ax, color='lime', markersize=2)\n",
    "\n",
    "gpd1int = gpd1.unary_union\n",
    "gpd2int = gpd2.unary_union\n",
    "\n",
    "intersection = gpd1int.intersection(gpd2int)\n",
    "\n",
    "#intersection = intersection[~intersection.is_empty]\n",
    "australia.plot(ax=ax, facecolor=\"None\", edgecolor='white')\n",
    "#gpd.GeoDataFrame(intersection).rename(columns={0:'geometry'}).set_geometry('geometry').plot(ax=ax, facecolor=\"None\", edgecolor='black', hatch=\"///\")\n",
    "gpd.GeoDataFrame(gpd.GeoSeries(intersection)).rename(columns={0:'geometry'}).set_geometry('geometry').plot(ax=ax, facecolor=\"None\", edgecolor='gray', hatch=\"///\")\n",
    "# Prepare for labels on GPD1\n",
    "gpd1['coords'] = gpd1['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "gpd1['coords'] = [coords[0] for coords in gpd1['coords']]\n",
    "for idx, row in gpd1.iterrows():\n",
    "    plt.annotate(s=row['Transit time                 :'], xy=row['coords'], horizontalalignment='center')\n",
    "# Plot it\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph it to confirm intersect of swath and hotspots (all won't match - not hotspots in the sea etc.)\n",
    "gpd1['Transit time                 :'] = pd.to_datetime(gpd1['Transit time                 :'])\n",
    "df = hotspots_sample[(hotspots_sample['satellite'] == satelliteA)].sort_values('datetime', ascending=True)\n",
    "dfswath = gpd1.sort_values('Transit time                 :', ascending=True)\n",
    "plt.plot(df['datetime'], df['datetime'], '*', color='blue', markersize=10)\n",
    "plt.plot(dfswath['Transit time                 :'], dfswath['Transit time                 :'], 'o', color='red', markersize=5)\n",
    "plt.xticks(rotation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_points['geometry'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of hotspot matches < 2000m\n",
    "numerator = pd.pivot_table(nearest_points[(nearest_points['dist_m'] < 2000)],values='count', index=['satellite_sensor_product'], columns=['2_satellite_sensor_product'], aggfunc={'count':len})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of hotspot matches - total\n",
    "denominator = pd.pivot_table(nearest_points,values='count', index=['2_satellite_sensor_product'], columns=['satellite_sensor_product'], aggfunc={'count':np.sum})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn styling for matrix\n",
    "denominatortable = denominator.style.format(\"{:g}\")\n",
    "cm = seaborn.light_palette(\"blue\", as_cmap=True)\n",
    "s = denominatortable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"matches_count.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of hotspot matches < 2000m\n",
    "numerator = pd.pivot_table(nearest_points[(nearest_points['dist_m'] < 2000)],values='count', index=['2_satellite_sensor_product'], columns=['satellite_sensor_product'], aggfunc={'count':len})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn styling for matrix\n",
    "numeratortable = numerator.style.format(\"{:g}\")\n",
    "cm = seaborn.light_palette(\"red\", as_cmap=True)\n",
    "s = numeratortable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"matches_2000m.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of matched points closer than 2000m\n",
    "difference = (denominator - numerator).style.format(\"{:g}\") \n",
    "# Set seaborn styling for matrix\n",
    "cm = seaborn.light_palette(\"red\", as_cmap=True)\n",
    "s = difference.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"count_difference.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of matched points closer than 2000m\n",
    "percentage = (numerator / denominator).style.format(\"{:.0%}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn styling for matrix\n",
    "cm = seaborn.light_palette(\"green\", as_cmap=True)\n",
    "s = percentage.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"percentage.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum time between matched points < 2000m\n",
    "timemax = pd.pivot_table(nearest_points[(nearest_points['dist_m'] < 2000)],values='timedelta', index=['satellite_sensor_product'], columns=['2_satellite_sensor_product'], aggfunc={'timedelta':np.max})\n",
    "# Set seaborn styling for matrix\n",
    "timemaxtable = timemax.style.format(\"{:}\")\n",
    "cm = seaborn.light_palette(\"gray\", as_cmap=True)\n",
    "s = timemaxtable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"max_time_matched_points.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render(annot=True))\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum time between matched points < 2000m\n",
    "timemin = pd.pivot_table(nearest_points[(nearest_points['dist_m'] < 2000)],values='timedelta', index=['2_satellite_sensor_product'], columns=['satellite_sensor_product'], aggfunc={'timedelta':np.min})\n",
    " \n",
    "# Set seaborn styling for matrix\n",
    "timemintable = timemin.style.format(\"{:}\")\n",
    "cm = seaborn.light_palette(\"purple\", as_cmap=True)\n",
    "s = timemintable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"min_time_matched_points.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average distance (m) between matched points < 2000m\n",
    "averagedist = pd.pivot_table(nearest_points[(nearest_points['dist_m'] < 2000)],values='dist_m', index=['2_satellite_sensor_product'], columns=['satellite_sensor_product'], aggfunc={'dist_m':np.mean})\n",
    "# Set seaborn styling for matrix\n",
    "averagedisttable = averagedist.style.format(\"{:}\")\n",
    "cm = seaborn.light_palette(\"olive\", as_cmap=True)\n",
    "s = averagedisttable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"avg_distance_2000m.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done - TODO polygon intersection (using satellite footprint tool and S3 geojson)\n",
    "# TODO match available Landsat and Sentinel 2 datasets with hotspots\n",
    "# TODO Use WCS to interact with GSKY for DEA data queries - https://gsky.readthedocs.io/en/latest/_notebook/Notebook_GSKY_WCS.html\n",
    "# TODO run fire detection on Landsat and Sentinel 2 - update attributes\n",
    "# Done - TODO rerun the nearest test with S3 as the primary, as well as DEA Hotspots.\n",
    "# Done - TODO add Himawari Hotspots from SRSS and WFABBA.(Done - see config.yaml, added password and request handling, concatenated GPD added)\n",
    "# Done - TODO add persistent hotspots and compare frequency of detection\n",
    "# TODO - get feedback on the length of day between midnight on the east side and midnight on the west side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions to answer\n",
    "# How to define coincidence?\n",
    "# How to constrain results - based on confidence? and minimum allowable radius?\n",
    "# Perhaps groupby time should be a moving window of two days or only look at matching hotspots first detected early in the day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Add moving window function i.e shift solar day by x hours forward or back and redo analysis\n",
    "# Done TODO - Add time delta for matches as well - average time delta might be a useful statistic\n",
    "# Done TODO - Add product group attribute to DEA dataframe\n",
    "# Done TODO roll this up to the nearest comparison to simplify product handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_points.to_csv('nearest_points.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - improve Results presentation\n",
    "# Plot time against count of matches\n",
    "# Plot vectors from one hotspot source to its match\n",
    "# TODO add vectors linking spots to their nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persistent_hotspots = gpd.GeoDataFrame.from_file('vectors/known_non_FHS.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO match DEA Hotspots to persistent hotspots\n",
    "# TODO match S3VT Hotspots to persistent hotspots\n",
    "# TODO how often do matched hotspots match with perisistent hotspots\n",
    "# TODO - improved logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent = ckdnearest( hotspots_gdf, persistent_hotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent['count'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent['dist_m'] = nearest_persistent.apply(lambda row: distance((row.latitude, row.longitude),(row['2_Latitude'], row['2_Longitude'])).meters, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of hotspot matches \n",
    "persistentcount = pd.pivot_table(nearest_persistent,values='count', index=['2_Comment'], columns=['satellite_sensor_product'], aggfunc={'count':np.sum})\n",
    "#persistentcount\n",
    "# Set seaborn styling for matrix\n",
    "persistentcounttable = persistentcount.style.format(\"{:g}\")\n",
    "cm = seaborn.light_palette(\"orange\", as_cmap=True)\n",
    "s = persistentcounttable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"persistent_matches.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count of hotspot matches < 2000m\n",
    "persistentcountm = pd.pivot_table(nearest_persistent[(nearest_persistent['dist_m'] < 2000)],values='count', index=['2_Comment'], columns=['satellite_sensor_product'], aggfunc={'count':np.sum})\n",
    "# Set seaborn styling for matrix\n",
    "persistentcountmtable = persistentcountm.style.format(\"{:g}\")\n",
    "cm = seaborn.light_palette(\"blue\", as_cmap=True)\n",
    "s = persistentcountmtable.background_gradient(cmap=cm)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = comparison_prefix+'-'+\"peristent_matches_2000m.html\"\n",
    "html = open(output,\"w\")\n",
    "html.write(s.render())\n",
    "html.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent['line_geometry'] = nearest_persistent.apply(lambda row: shapely.wkt.loads(LineString([row.geometry, row['2_geometry']]).wkt), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent = nearest_persistent.set_geometry('line_geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_persistent.geometry.plot(figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gjson = persistent_hotspots.geometry.to_crs(epsg='4326').to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapa = folium.Map([-26, 132],\n",
    "                  zoom_start=4,\n",
    "                  tiles='Stamen Terrain')\n",
    "\n",
    "points = folium.features.GeoJson(gjson)\n",
    "\n",
    "mapa.add_child(points)\n",
    "mapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
