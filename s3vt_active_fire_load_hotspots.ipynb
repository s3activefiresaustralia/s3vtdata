{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from netCDF4 import Dataset\n",
    "import datetime as dt\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "import xmltodict\n",
    "import yaml\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "import os\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from shapely.geometry import Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'config.yaml') as file:\n",
    "    configuration =  yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in configuration['configurations']:\n",
    "    print(config)\n",
    "    username = config['username']\n",
    "    password = config['password']\n",
    "    url = config['url']\n",
    "    aoi = config['aoi']\n",
    "    awss3bucket = config['awss3bucket']\n",
    "    awskeyid = config['awskeyid']\n",
    "    awskeypass = config['awskeypass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(sensors):\n",
    "\n",
    "    for sensordict in sensors:\n",
    "        \n",
    "        filter_string = ''\n",
    "        count = 0\n",
    "        \n",
    "        for sensor in sensordict.keys():\n",
    "            filter_string = filter_string+'(sensor=%27'+sensor+'%27%20AND%20(product=%27'\n",
    "            product_count = 0\n",
    "            for product in sensordict[sensor]:\n",
    "                filter_string = filter_string+product+'%27'\n",
    "                if product_count < (len(sensordict[sensor])-1):\n",
    "                    filter_string = filter_string+'%20OR%20product=%27'\n",
    "                else:\n",
    "                    filter_string = filter_string+'))' \n",
    "                product_count = product_count + 1\n",
    "            if count < (len(sensordict.keys())-1):        \n",
    "                filter_string = filter_string+'%20OR%20'\n",
    "            count = count+1\n",
    "\n",
    "    return(filter_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polygon_from_gml(gml_dict):\n",
    "    listoftuples = []\n",
    "    for i in list(gml_dict.split(\" \")):\n",
    "        pair = (float(i.split(',')[1]), float(i.split(',')[0]))\n",
    "        listoftuples.append(pair)\n",
    "    return(listoftuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hotspots(filter_string, time_period, bbox, max_features, min_confidence, to_date):\n",
    "    y_max = bbox[0]\n",
    "    x_min = bbox[1]\n",
    "    y_min = bbox[2]\n",
    "    x_max = bbox[3]\n",
    "    if to_date is None:\n",
    "        \n",
    "        to_date = dt.datetime.now()\n",
    "    \n",
    "    \n",
    "    from_date = (to_date - dt.timedelta(days=time_period)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # trim datetime to enable WFS \n",
    "    to_date = to_date.strftime('%Y-%m-%d')\n",
    "    logger.info(str(from_date)+' '+str(to_date))\n",
    "    \n",
    "    # TODO - sort out paging - looks like there is a limit to WFS requests number returned per query\n",
    "    logger.info(f\"https://hotspots.dea.ga.gov.au/geoserver/public/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=public:hotspots&outputFormat=application/json&CQL_FILTER=({filter_string})%20AND%20datetime%20%3E%20%27{from_date}%27%20AND%20datetime%20%3C%20%27{to_date}%27%20AND%20INTERSECTS(location,%20POLYGON(({y_max}%20{x_min},%20{y_max}%20{x_max},%20{y_min}%20{x_max},%20{y_min}%20{x_min},%20{y_max}%20{x_min})))&maxFeatures={max_features}&startIndex=0&sortBy=sensor%20A\")\n",
    "    url = f\"https://hotspots.dea.ga.gov.au/geoserver/public/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=public:hotspots&outputFormat=application/json&CQL_FILTER=({filter_string})%20AND%20datetime%20%3E%20%27{from_date}%27%20AND%20datetime%20%3C%20%27{to_date}%27%20AND%20INTERSECTS(location,%20POLYGON(({y_max}%20{x_min},%20{y_max}%20{x_max},%20{y_min}%20{x_max},%20{y_min}%20{x_min},%20{y_max}%20{x_min})))&maxFeatures={max_features}&startIndex=0&sortBy=sensor%20A\"\n",
    "    \n",
    "    hotspots_gdf = gpd.read_file(url)\n",
    "    #logger.info(str(hotspots_gdf['stop_dt']))\n",
    "    \n",
    "    # TODO - improved None value handling  -currently just look at first and apply that to all\n",
    "    if hotspots_gdf['confidence'][0] == None:\n",
    "        logger.info('Skipping confidence filter as confidence not populated')\n",
    "    else:\n",
    "\n",
    "        # Filter by confidence\n",
    "        hotspots_gdf = hotspots_gdf.loc[hotspots_gdf.confidence >= min_confidence]\n",
    "\n",
    "    # Fix datetime\n",
    "    if hotspots_gdf['start_dt'][0] == None:\n",
    "        logger.info('Start date field is not populated')\n",
    "        hotspots_gdf['datetime'] = pd.to_datetime(hotspots_gdf['datetime'])\n",
    "    else:\n",
    "        hotspots_gdf['datetime'] = pd.to_datetime(hotspots_gdf['start_dt'])\n",
    "\n",
    "    # Extract required columns\n",
    "    hotspots_gdf = hotspots_gdf.loc[:, [\n",
    "            'datetime', 'latitude', 'longitude', 'confidence', 'geometry', 'product', 'satellite', 'sensor', 'power'\n",
    "            ]]\n",
    "    hotspots_gdf.sort_values('datetime', ascending=True, inplace=True)\n",
    "    logger.info('Hotspots loaded successfully '+str(hotspots_gdf.geometry.total_bounds))\n",
    "\n",
    "    return(hotspots_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solar_day(utc, latitude):\n",
    "    SECONDS_PER_DEGREE = 240\n",
    "    offset_seconds = int(latitude * SECONDS_PER_DEGREE)\n",
    "    offset = np.timedelta64(offset_seconds, 's')\n",
    "    return (np.datetime64(utc) + offset).astype(datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess inventory against AWS bucket listing\n",
    "\n",
    "s3 = boto3.resource('s3', aws_access_key_id=awskeyid,\n",
    "                    aws_secret_access_key=awskeypass)\n",
    "\n",
    "s3folderlist = []\n",
    "s3geojsonlist = []\n",
    "s3bucket = s3.Bucket('s3vtaustralia')\n",
    "\n",
    "for bucket_object in s3bucket.objects.all():\n",
    "    s3bucketobject = str(bucket_object.key).split(\"/\")[2]\n",
    "    if '.SEN3' in s3bucketobject:\n",
    "        s3folderlist.append(s3bucketobject)\n",
    "    if '.FRP.geojson' in s3bucketobject:\n",
    "        s3geojsonlist.append(bucket_object.key)\n",
    "\n",
    "print(len(s3geojsonlist), \"S3 Hotspot files From ESA available in AWS S3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(s3folderlist)), \"S3 files From ESA available in AWS S3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read inventory to geopandas - write to geojson       \n",
    "        \n",
    "with open('s3vt_inventory.json') as inventory:\n",
    "    frames = []\n",
    "    for p in inventory:\n",
    "        pages = json.loads(p)\n",
    "                \n",
    "        for page in pages:\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    \n",
    "                    for entry in page['feed']['entry']:\n",
    "\n",
    "                        df = pd.DataFrame.from_dict(entry, orient='index')\n",
    "\n",
    "                        polygon = get_polygon_from_gml(xmltodict.parse(entry['str'][2]['content'])['gml:Polygon']['gml:outerBoundaryIs']['gml:LinearRing']['gml:coordinates'])\n",
    "\n",
    "                        df = df.transpose()\n",
    "                        df['Coordinates'] = Polygon(polygon)\n",
    "                        for d in entry['str']:\n",
    "                            if d['name'] ==  'orbitdirection':\n",
    "                                df['orbitdirection'] = d['content']\n",
    "                            if d['name'] ==  'platformidentifier':\n",
    "                                df['platformidentifier'] = d['content'] \n",
    "                            if d['name'] ==  'filename':\n",
    "                                df['filename'] = d['content']\n",
    "                            if d['name'] ==  'instrumentshortname':\n",
    "                                df['instrumentshortname'] = d['content']\n",
    "                            if d['name'] ==  'passnumber':\n",
    "                                df['passnumber'] = d['content']        \n",
    "                        s3vtdf = gpd.GeoDataFrame(df, geometry='Coordinates')\n",
    "\n",
    "                        frames.append(s3vtdf)\n",
    "                                \n",
    "                except KeyError:\n",
    "                        logger.info(\"KeyError exception for get_polygon_from_gml()\")\n",
    "                        \n",
    "s3vtgpd = pd.concat(frames)\n",
    "\n",
    "# Not sure why we need to index but do it anyway\n",
    "s3vtgpd = s3vtgpd.reset_index(drop=True)\n",
    "s3vtgpd['date'] = pd.to_datetime(s3vtgpd.summary.str.split(\",\", expand= True)[0].str.split(' ', expand=True)[1])\n",
    "# Some fields are lists and geojson translation doesn't like it\n",
    "\n",
    "s3vtgpd = s3vtgpd.drop(['link', 'int', 'str', 'summary'], axis=1)\n",
    "s3vtgpd.to_file('s3vt_geometry.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframelength = len(s3vtgpd)\n",
    "# Add field to enable monitoring\n",
    "s3vtgpd['hotspot'] = 0\n",
    "s3vtgpd['download'] = 0\n",
    "s3vtgpd['s3bucket'] = 0\n",
    "\n",
    "s3vthostpotsgpdlist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if folder already downloaded and flag in gpd\n",
    "for i in range(dataframelength):\n",
    "    if s3vtgpd.loc[i]['title']+'.SEN3' in set(s3folderlist):\n",
    "        s3vtgpd.at[i, 'download'] = 1\n",
    "    if s3vtgpd.loc[i]['title']+'.FRP.geojson' in set(s3folderlist):\n",
    "        s3vtgpd.at[i, 'hotspot'] = 1\n",
    "        s3vthostpotsgpdlist.append(s3hotspotsgpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all S3 geojson files locally and load to GPD\n",
    "\n",
    "for i in s3geojsonlist:\n",
    "    print(i)\n",
    "    subprocess.call(['aws', 's3', 'cp', 's3://s3vtaustralia/'+i, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for i in s3geojsonlist:\n",
    "    df1 = gpd.read_file(i)\n",
    "    if 'S3A' in i:\n",
    "        df1['satellite'] = 'S3A'\n",
    "    else:\n",
    "        df1['satellite'] = 'S3B'\n",
    "    df1['sensor'] = 'SLSTR'\n",
    "    df2 = df1.query(\"FRP_MWIR>0\")\n",
    "    if len(df2) > 0:\n",
    "        frames.append(df2)\n",
    "        \n",
    "s3vthotspots = pd.concat(frames)\n",
    "#for i in s3geojsonlist:\n",
    "#    s3vtdf = gpd.GeoDataFrame(df, geometry='Coordinates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots['date'] = pd.to_datetime(netCDF4.num2date(s3vthotspots.time, units='microseconds since 2000-01-01T00:00:00Z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots.to_file('s3vt_hotspots.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = list(s3vthotspots.geometry.total_bounds)\n",
    "bbox = [bounds[3], bounds[0], bounds[1], bounds[2]]\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get time bounds for DEA Hotspots query\n",
    "# TODO - AHI hotspots\n",
    "\n",
    "maxdate = s3vthotspots.date.max().to_datetime64()\n",
    "mindate = s3vthotspots.date.min().to_datetime64()\n",
    "to_date = dt.datetime.strptime(str(s3vthotspots.date.max().to_datetime64()), '%Y-%m-%dT%H:%M:%S.%f000') # '2018-01-01T00:00:00.000Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_period = int(str(np.timedelta64(maxdate - mindate, 'D')).split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in configuration['configurations']:\n",
    "    \n",
    "    hotspots_gdf = load_hotspots(filter(config['sensors']),\n",
    "                                         time_period,\n",
    "                                         bbox,\n",
    "                                         config['max_features'], \n",
    "                                         config['min_confidence'],\n",
    "                                         to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('DEAHotspots_hotspots.geojson')\n",
    "hotspots_gdf.to_file('DEAHotspots_hotspots.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf.datetime.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Hotspots to GeoPandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf = gpd.read_file('DEAHotspots_hotspots.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots = gpd.read_file('s3vt_hotspots.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ckdnearest(gdA, gdB):\n",
    "    nA = np.array(list(zip(gdA.geometry.x, gdA.geometry.y)) )\n",
    "    nB = np.array(list(zip(gdB.geometry.x, gdB.geometry.y)) )\n",
    "    btree = cKDTree(nB)\n",
    "    dist, idx = btree.query(nA, k=1)\n",
    "    gdf = pd.concat(\n",
    "        [gdA.reset_index(drop=True), gdB.loc[idx, gdB.columns != 'geometry'].reset_index(drop=True),\n",
    "         pd.Series(dist, name='dist')], axis=1)\n",
    "    return gdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "products = [{'product': 'SRSS', 'sensor': 'MODIS'},\n",
    " {'product': 'MOD14', 'sensor': 'MODIS'},\n",
    " {'product': 'SRSS', 'satellite': 'AVHRR'},\n",
    " {'product': 'GA', 'satellite': 'AVHRR'},\n",
    " {'product': 'SRSS', 'satellite': 'VIIRS'},\n",
    " {'product': 'GA', 'satellite': 'VIIRS'}]\n",
    " \n",
    "ckdnearest(hotspots_gdf, s3vthotspots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO - group temporally - same solar day\n",
    "#TODO - find nearest\n",
    "# https://stackoverflow.com/questions/18737942/clustering-in-pythonscipy-with-space-and-time-variables\n",
    "\n",
    "'''\n",
    "\n",
    "You'll need to define your own metric, which handles \"time\" in an appropriate way. In the docs for scipy.spatial.distance.pdist you can define your own function\n",
    "\n",
    "Y = pdist(X, f)\n",
    "\n",
    "    Computes the distance between all pairs of vectors in X using the user supplied 2-arity function f. [...] For example, Euclidean distance between the vectors could be computed as follows:\n",
    "\n",
    "dm = pdist(X, lambda u, v: np.sqrt(((u-v)**2).sum()))\n",
    "\n",
    "The metric can be passed to any scipy clustering algorithm, via the metric keyword. For example, using linkage:\n",
    "\n",
    "scipy.cluster.hierarchy.linkage(y, method='single', metric='euclidean')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf['solar_day'] = hotspots_gdf.apply(lambda row: solar_day(row.datetime, row.latitude), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots['solar_day'] = s3vthotspots.apply(lambda row: solar_day(row.datetime, row.latitude), axis = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solar_day(hotspots_gdf.datetime[0],hotspots_gdf.latitude[0] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
