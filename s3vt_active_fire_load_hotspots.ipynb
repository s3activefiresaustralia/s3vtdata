{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "from netCDF4 import Dataset\n",
    "import datetime as dt\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon\n",
    "import xmltodict\n",
    "import yaml\n",
    "import os\n",
    "import subprocess\n",
    "from datetime import date\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "import os\n",
    "import netCDF4\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.setLevel(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'config.yaml') as file:\n",
    "    configuration =  yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in configuration['configurations']:\n",
    "    print(config)\n",
    "    username = config['username']\n",
    "    password = config['password']\n",
    "    url = config['url']\n",
    "    aoi = config['aoi']\n",
    "    awss3bucket = config['awss3bucket']\n",
    "    awskeyid = config['awskeyid']\n",
    "    awskeypass = config['awskeypass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(sensors):\n",
    "\n",
    "    for sensordict in sensors:\n",
    "        \n",
    "        filter_string = ''\n",
    "        count = 0\n",
    "        \n",
    "        for sensor in sensordict.keys():\n",
    "            filter_string = filter_string+'(sensor=%27'+sensor+'%27%20AND%20(product=%27'\n",
    "            product_count = 0\n",
    "            for product in sensordict[sensor]:\n",
    "                filter_string = filter_string+product+'%27'\n",
    "                if product_count < (len(sensordict[sensor])-1):\n",
    "                    filter_string = filter_string+'%20OR%20product=%27'\n",
    "                else:\n",
    "                    filter_string = filter_string+'))' \n",
    "                product_count = product_count + 1\n",
    "            if count < (len(sensordict.keys())-1):        \n",
    "                filter_string = filter_string+'%20OR%20'\n",
    "            count = count+1\n",
    "\n",
    "    return(filter_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polygon_from_gml(gml_dict):\n",
    "    listoftuples = []\n",
    "    for i in list(gml_dict.split(\" \")):\n",
    "        pair = (float(i.split(',')[1]), float(i.split(',')[0]))\n",
    "        listoftuples.append(pair)\n",
    "    return(listoftuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hotspots(filter_string, time_period, bbox, max_features, min_confidence, to_date):\n",
    "    y_max = bbox[0]\n",
    "    x_min = bbox[1]\n",
    "    y_min = bbox[2]\n",
    "    x_max = bbox[3]\n",
    "    if to_date is None:\n",
    "        \n",
    "        to_date = dt.datetime.now()\n",
    "    \n",
    "    \n",
    "    from_date = (to_date - dt.timedelta(days=time_period)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # trim datetime to enable WFS \n",
    "    to_date = to_date.strftime('%Y-%m-%d')\n",
    "    logger.info(str(from_date)+' '+str(to_date))\n",
    "    \n",
    "    # TODO - sort out paging - looks like there is a limit to WFS requests number returned per query\n",
    "    logger.info(f\"https://hotspots.dea.ga.gov.au/geoserver/public/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=public:hotspots&outputFormat=application/json&CQL_FILTER=({filter_string})%20AND%20datetime%20%3E%20%27{from_date}%27%20AND%20datetime%20%3C%20%27{to_date}%27%20AND%20INTERSECTS(location,%20POLYGON(({y_max}%20{x_min},%20{y_max}%20{x_max},%20{y_min}%20{x_max},%20{y_min}%20{x_min},%20{y_max}%20{x_min})))&maxFeatures={max_features}&startIndex=0&sortBy=sensor%20A\")\n",
    "    url = f\"https://hotspots.dea.ga.gov.au/geoserver/public/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=public:hotspots&outputFormat=application/json&CQL_FILTER=({filter_string})%20AND%20datetime%20%3E%20%27{from_date}%27%20AND%20datetime%20%3C%20%27{to_date}%27%20AND%20INTERSECTS(location,%20POLYGON(({y_max}%20{x_min},%20{y_max}%20{x_max},%20{y_min}%20{x_max},%20{y_min}%20{x_min},%20{y_max}%20{x_min})))&maxFeatures={max_features}&startIndex=0&sortBy=sensor%20A\"\n",
    "    \n",
    "    hotspots_gdf = gpd.read_file(url)\n",
    "    #logger.info(str(hotspots_gdf['stop_dt']))\n",
    "    \n",
    "    # TODO - improved None value handling  -currently just look at first and apply that to all\n",
    "    if hotspots_gdf['confidence'][0] == None:\n",
    "        logger.info('Skipping confidence filter as confidence not populated')\n",
    "    else:\n",
    "\n",
    "        # Filter by confidence\n",
    "        hotspots_gdf = hotspots_gdf.loc[hotspots_gdf.confidence >= min_confidence]\n",
    "\n",
    "    # Fix datetime\n",
    "    if hotspots_gdf['start_dt'][0] == None:\n",
    "        logger.info('Start date field is not populated')\n",
    "        hotspots_gdf['datetime'] = pd.to_datetime(hotspots_gdf['datetime'])\n",
    "    else:\n",
    "        hotspots_gdf['datetime'] = pd.to_datetime(hotspots_gdf['start_dt'])\n",
    "\n",
    "    # Extract required columns\n",
    "    hotspots_gdf = hotspots_gdf.loc[:, [\n",
    "            'datetime', 'latitude', 'longitude', 'confidence', 'geometry', 'product', 'satellite', 'sensor', 'power'\n",
    "            ]]\n",
    "    hotspots_gdf.sort_values('datetime', ascending=True, inplace=True)\n",
    "    logger.info('Hotspots loaded successfully '+str(hotspots_gdf.geometry.total_bounds))\n",
    "\n",
    "    return(hotspots_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197 S3 Hotspot files From ESA available in AWS S3\n"
     ]
    }
   ],
   "source": [
    "# Assess inventory against AWS bucket listing\n",
    "\n",
    "s3 = boto3.resource('s3', aws_access_key_id=awskeyid,\n",
    "                    aws_secret_access_key=awskeypass)\n",
    "\n",
    "s3folderlist = []\n",
    "s3geojsonlist = []\n",
    "s3bucket = s3.Bucket('s3vtaustralia')\n",
    "\n",
    "for bucket_object in s3bucket.objects.all():\n",
    "    s3bucketobject = str(bucket_object.key).split(\"/\")[2]\n",
    "    if '.SEN3' in s3bucketobject:\n",
    "        s3folderlist.append(s3bucketobject)\n",
    "    if '.FRP.geojson' in s3bucketobject:\n",
    "        s3geojsonlist.append(bucket_object.key)\n",
    "\n",
    "print(len(s3geojsonlist), \"S3 Hotspot files From ESA available in AWS S3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read inventory to geopandas - write to geojson       \n",
    "        \n",
    "with open('s3vt_inventory.json') as inventory:\n",
    "    frames = []\n",
    "    for p in inventory:\n",
    "        pages = json.loads(p)\n",
    "                \n",
    "        for page in pages:\n",
    "            for entry in page['feed']['entry']:\n",
    "                        \n",
    "                df = pd.DataFrame.from_dict(entry, orient='index')\n",
    "                        \n",
    "                polygon = get_polygon_from_gml(xmltodict.parse(entry['str'][2]['content'])['gml:Polygon']['gml:outerBoundaryIs']['gml:LinearRing']['gml:coordinates'])\n",
    "                \n",
    "                df = df.transpose()\n",
    "                df['Coordinates'] = Polygon(polygon)\n",
    "                for d in entry['str']:\n",
    "                    if d['name'] ==  'orbitdirection':\n",
    "                        df['orbitdirection'] = d['content']\n",
    "                    if d['name'] ==  'platformidentifier':\n",
    "                        df['platformidentifier'] = d['content'] \n",
    "                    if d['name'] ==  'filename':\n",
    "                        df['filename'] = d['content']\n",
    "                    if d['name'] ==  'instrumentshortname':\n",
    "                        df['instrumentshortname'] = d['content']\n",
    "                    if d['name'] ==  'passnumber':\n",
    "                        df['passnumber'] = d['content']        \n",
    "                s3vtdf = gpd.GeoDataFrame(df, geometry='Coordinates')\n",
    "                \n",
    "                frames.append(s3vtdf) \n",
    "                    \n",
    "s3vtgpd = pd.concat(frames)\n",
    "\n",
    "# Not sure why we need to index but do it anyway\n",
    "s3vtgpd = s3vtgpd.reset_index(drop=True)\n",
    "s3vtgpd['date'] = pd.to_datetime(s3vtgpd.summary.str.split(\",\", expand= True)[0].str.split(' ', expand=True)[1])\n",
    "# Some fields are lists and geojson translation doesn't like it\n",
    "\n",
    "s3vtgpd = s3vtgpd.drop(['link', 'int', 'str', 'summary'], axis=1)\n",
    "s3vtgpd.to_file('s3vt_geometry.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframelength = len(s3vtgpd)\n",
    "# Add field to enable monitoring\n",
    "s3vtgpd['hotspot'] = 0\n",
    "s3vtgpd['download'] = 0\n",
    "s3vtgpd['s3bucket'] = 0\n",
    "\n",
    "s3vthostpotsgpdlist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if folder already downloaded and flag in gpd\n",
    "for i in range(dataframelength):\n",
    "    if s3vtgpd.loc[i]['title']+'.SEN3' in set(s3folderlist):\n",
    "        s3vtgpd.at[i, 'download'] = 1\n",
    "    if s3vtgpd.loc[i]['title']+'.FRP.geojson' in set(s3folderlist):\n",
    "        s3vtgpd.at[i, 'hotspot'] = 1\n",
    "        s3vthostpotsgpdlist.append(s3hotspotsgpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy all S3 geojson files locally and load to GPD\n",
    "\n",
    "for i in s3geojsonlist:\n",
    "    subprocess.call(['aws', 's3', 'cp', 's3://s3vtaustralia/'+i, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for i in s3geojsonlist:\n",
    "    df1 = gpd.read_file(i)\n",
    "    if 'S3A' in i:\n",
    "        df1['satellite'] = 'S3A'\n",
    "    else:\n",
    "        df1['satellite'] = 'S3B'\n",
    "    df1['sensor'] = 'SLSTR'\n",
    "    df2 = df1.query(\"FRP_MWIR>0\")\n",
    "    if len(df2) > 0:\n",
    "        frames.append(df2)\n",
    "        \n",
    "s3vthotspots = pd.concat(frames)\n",
    "#for i in s3geojsonlist:\n",
    "#    s3vtdf = gpd.GeoDataFrame(df, geometry='Coordinates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots['date'] = pd.to_datetime(netCDF4.num2date(s3vthotspots.time, units='microseconds since 2000-01-01T00:00:00Z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3vthotspots.to_file('s3vt_hotspots.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11.751792970329575,\n",
       " -171.17838656781612,\n",
       " -44.941232886534294,\n",
       " 176.88213855341553]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bounds = list(s3vthotspots.geometry.total_bounds)\n",
    "bbox = [bounds[3], bounds[0], bounds[1], bounds[2]]\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get time bounds for DEA Hotspots query\n",
    "# TODO - AHI hotspots\n",
    "\n",
    "maxdate = s3vthotspots.date.max().to_datetime64()\n",
    "mindate = s3vthotspots.date.min().to_datetime64()\n",
    "to_date = dt.datetime.strptime(str(s3vthotspots.date.max().to_datetime64()), '%Y-%m-%dT%H:%M:%S.%f000') # '2018-01-01T00:00:00.000Z'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_period = int(str(np.timedelta64(maxdate - mindate, 'D')).split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:2020-02-27 2020-03-01\n",
      "INFO:root:https://hotspots.dea.ga.gov.au/geoserver/public/wfs?service=WFS&version=1.1.0&request=GetFeature&typeName=public:hotspots&outputFormat=application/json&CQL_FILTER=((sensor=%27AVHRR%27%20AND%20(product=%27SRSS%27%20OR%20product=%27GA%27))%20OR%20(sensor=%27MODIS%27%20AND%20(product=%27MOD14%27%20OR%20product=%27SRSS%27))%20OR%20(sensor=%27VIIRS%27%20AND%20(product=%27AFMOD%27%20OR%20product=%27EDR%27)))%20AND%20datetime%20%3E%20%272020-02-27%27%20AND%20datetime%20%3C%20%272020-03-01%27%20AND%20INTERSECTS(location,%20POLYGON((11.751792970329575%20-171.17838656781612,%2011.751792970329575%20176.88213855341553,%20-44.941232886534294%20176.88213855341553,%20-44.941232886534294%20-171.17838656781612,%2011.751792970329575%20-171.17838656781612)))&maxFeatures=500000&startIndex=0&sortBy=sensor%20A\n",
      "INFO:root:Hotspots loaded successfully [101.06687927 -44.438      176.81         7.913     ]\n"
     ]
    }
   ],
   "source": [
    "for config in configuration['configurations']:\n",
    "    \n",
    "    hotspots_gdf = load_hotspots(filter(config['sensors']),\n",
    "                                         time_period,\n",
    "                                         bbox,\n",
    "                                         config['max_features'], \n",
    "                                         config['min_confidence'],\n",
    "                                         to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('DEAHotspots_hotspots.geojson')\n",
    "hotspots_gdf.to_file('DEAHotspots_hotspots.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-02-29 23:38:50')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hotspots_gdf.datetime.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2020-03-01 15:24:53.078015')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3vthotspots.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
