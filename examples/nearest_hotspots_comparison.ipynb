{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "import logging\n",
    "import uuid\n",
    "import time\n",
    "import shapely\n",
    "import pandas as pd\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "import geopandas as gpd\n",
    "import seaborn\n",
    "import dask\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "import src.hotspot_utils as util \n",
    "import src.process_nearest_hotspots as nearest_process\n",
    "import src.xml_util as xutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = Path(f\"/home/jovyan/s3vt_dask/s3vtdata/workdir_test1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    ")\n",
    "_LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include n_workers equal or less than the number of core\n",
    "# To visualise status in Dask add /user/<username>/proxy/8787/status\n",
    "#client = Client(memory_limit='7GB', n_workers=8, processes=False ) - no S3B low memory use\n",
    "#client = Client(memory_limit='7GB', n_workers=8, threads_per_worker=2 )\n",
    "client = Client( n_workers=4 )\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Parameter used in Sub-setting Spatial Extent and Temporal Range for Area of Interest\n",
    "##### The FRP data from nasa, esa, eumetsat and landgate are merged, sub-setted and neareast hotspots csv files are generated based on the parameters in `processing_parameters`  \n",
    "##### The parameter `chunks` in blocking FRP data to enable multi-processing. If you encounter memory issues then higher the number.\n",
    "##### The `start_time` and `end_time` can be used to subset for solar_day (3:00-22:00), solar_night (22:00-03:00 with 12 hours offset) and solar_all(0:00-24:00) hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_parameters = {\n",
    "    \"nasa_frp\": \"s3://s3vtaustralia/nasa_hotspots_gdf.geojson\",\n",
    "    \"esa_frp\": \"s3://s3vtaustralia/s3vt_hotspots.geojson\",\n",
    "    \"eumetsat_frp\": \"s3://s3vtaustralia/s3vt_eumetsat_hotspots.geojson\",\n",
    "    \"landgate_frp\": \"s3://s3vtaustralia/landgate_hotspots_gdf.geojson\",\n",
    "    \"sentinel3_swath_geojson\": \"s3://s3vtaustralia/sentinel3_swath_gdfs.geojson\",\n",
    "    \"dea_frp\": None,\n",
    "    \"lon_west\": 113.0, #147.0,\n",
    "    \"lat_south\": -44, #-38.0,\n",
    "    \"lon_east\": 154.0,\n",
    "    \"lat_north\": -10, #-27.,\n",
    "    \"start_date\":  \"2020-02-02\", #\"2019-11-01\",\n",
    "    \"end_date\": \"2020-10-08\",\n",
    "    \"start_time\": \"20:00\",\n",
    "    \"end_time\": \"03:00\",\n",
    "    \"chunks\": 250,\n",
    "    \"compare_field\": \"solar_night\",\n",
    "    \"swath_config_file\": Path(\"/home/jovyan/s3vt_dask/s3vtdata/configs/s3vtconfig.yaml\"),\n",
    "    \"outdir\": outdir,\n",
    "    \"test\": False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to generate nearest .csv files. If .csv files already exists then skip this process. Takes around ~5-6 hours in this sandbox environment with 2-core and 16 GB RAM\n",
    "nearest_hotspots_product_files = nearest_process.process_nearest_points(**processing_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Hotspots DataFrame merged from neareast hotspots csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv directory is where nearest hotspots csv files are stored. \n",
    "csv_directory = outdir\n",
    "print(csv_directory)\n",
    "# This is read all the .csv files if name starts with `nearest_points` and ends with `compare_field` value from processing parameters.\n",
    "nearest_hotspots_csv_files = [\n",
    "    fp for fp in csv_directory.iterdir()\n",
    "    if (fp.name.startswith(\"nearest_points\"))\n",
    "    and (fp.name.endswith(\"csv\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional if full run is broken into smaller timeperiods\n",
    "#import glob\n",
    "# relative path to search all text files\n",
    "#files = glob.glob(\"/home/jovyan/s3vt_dask/s3vtdata/workdir_test1/20*/*\")\n",
    "#nearest_hotspots_csv_files = []\n",
    "#for i in files:\n",
    "#    nearest_hotspots_csv_files.append(Path(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nearest_hotspots_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for bad csvs\n",
    "#csv_directory = Path(f\"/home/jovyan/s3vt_dask/s3vtdata/workdir_test1/20191101_20200201/\")\n",
    "#nearest_hotspots_csv_files2 = [\n",
    "#    fp for fp in csv_directory.iterdir()\n",
    "#    if (fp.name.startswith(\"nearest_points\"))\n",
    "#    and (fp.name.endswith(\"csv\"))\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest points csv files that will be used analysis from here on.\n",
    "nearest_hotspots_csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a dask DataFrame with index set at column `compare_field` from processing_parameters.\n",
    "nearest_points_ddf = util.csv_to_dataframe(nearest_hotspots_csv_files, processing_parameters[\"compare_field\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_points_ddf = nearest_points_ddf.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_sensor_product = nearest_points_ddf.satellite_sensor_product.unique()\n",
    "satellite_sensor_product_daily_list = []\n",
    "for i in satellite_sensor_product:\n",
    "    \n",
    "    for j in satellite_sensor_product:\n",
    "        if i != j:\n",
    "            Acount = nearest_points_ddf[(nearest_points_ddf['satellite_sensor_product'] == i) & (nearest_points_ddf['2_satellite_sensor_product'] == j)].resample(\"D\", on='datetime').datetime.count()\n",
    "            Acount = Acount.rename('main_count')\n",
    "            Bcount = nearest_points_ddf[(nearest_points_ddf['satellite_sensor_product'] == j) & (nearest_points_ddf['2_satellite_sensor_product'] == i)].resample(\"D\", on='datetime').datetime.count()\n",
    "            Bcount = Bcount.rename('secondary_count')\n",
    "            \n",
    "            # Use the ratio of the master and slave daily counts in the same observation extent as a consistency measure\n",
    "            # - theory being that there should be a proportional shift in both hotspot sources\n",
    "            # - using ratio as a quick measure\n",
    "            \n",
    "            ratio = Acount/Bcount\n",
    "            ratio = ratio.rename('ratio')\n",
    "           \n",
    "            zscore = ratio - ratio.mean() / ratio.std()\n",
    "            zscore = zscore.rename('zscore')\n",
    "\n",
    "            satellite_sensor_product_daily = pd.concat([Acount, Bcount, ratio, zscore], axis=1)\n",
    "            satellite_sensor_product_daily['satellite_sensor_product'] = i\n",
    "            satellite_sensor_product_daily['2_satellite_sensor_product'] = j\n",
    "            satellite_sensor_product_daily['pair'] = i+'|'+j\n",
    "            satellite_sensor_product_daily_list.append(satellite_sensor_product_daily)\n",
    "            print(i,j,ratio.max(),ratio.std(),ratio.mean())\n",
    "satellite_sensor_product_daily_pd = pd.concat(satellite_sensor_product_daily_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_sensor_product_daily_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_sensor_product_daily_pd.zscore.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list(satellite_sensor_product_daily_pd['pair'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_sensor_product_daily.satellite_sensor_product.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_list = []\n",
    "pair_dict = {}\n",
    "for pair in pairs:\n",
    "    daily_pair = satellite_sensor_product_daily_pd[satellite_sensor_product_daily_pd['pair'] == pair]\n",
    "    correlation = daily_pair[['main_count','secondary_count']].corr().main_count[1]\n",
    "    #pair_list.append(pd.DataFrame({pair: {'correlation': correlation}}))\n",
    "    pair_dict[pair] = {'correlation': correlation, 'main': pair.split('|')[0], 'secondary': pair.split('|')[1]}\n",
    "pair_pd = pd.DataFrame(pair_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for main in pair_pd['main'].unique():\n",
    "    print(pair_pd[pair_pd['main'] == main][['correlation']].sort_values(by=['correlation'], ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = satellite_sensor_product_daily_pd[(satellite_sensor_product_daily_pd['pair'] == 'NOAA 20_VIIRS_NASA2.0NRT|SENTINEL_3A_SLSTR_EUMETSAT')]\n",
    "#sample = satellite_sensor_product_daily_pd[(satellite_sensor_product_daily_pd['satellite_sensor_product'] == 'SUOMI NPP_VIIRS_LANDGATE') & (satellite_sensor_product_daily_pd['2_satellite_sensor_product'] == 'SUOMI NPP_VIIRS_NASA1')]#.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotly.write_image('daily_count_comparison')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut and paste pair from above to visualise daily pair counts (from coincident observation area)\n",
    "# Theory is that within in some range, the number of hotspots should be roughly proportional, day to day.\n",
    "pair = 'SENTINEL_3B_SLSTR_EUMETSAT|NOAA 20_VIIRS_NASA2.0NRT'\n",
    "\n",
    "example = satellite_sensor_product_daily_pd[satellite_sensor_product_daily_pd['pair'] == pair]\n",
    "ax = example.plot.scatter(x=example.index, y=['main_count', 'secondary_count'], title=pair)\n",
    "ax.layout.xaxis = {    'anchor': 'y', 'domain': [0.0, 1.0], 'title': {'text': 'date'}}\n",
    "ax.layout.yaxis = {    'anchor': 'x', 'domain': [0.0, 1.0], 'title': {'text': 'hotspots'}}\n",
    "#ax.layout.legend = {    'title': {'satellite_sensor_product': 'variable'}, 'tracegroupgap': 0}\n",
    "ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_candidates_list = []\n",
    "for i in satellite_sensor_product_daily_pd['pair'].unique():\n",
    "    candidate_pair = satellite_sensor_product_daily_pd[(satellite_sensor_product_daily_pd['pair'] == i)]\n",
    "    filter_candidates_list.append(pd.DataFrame({'pair': i, \n",
    "                                                'satellite_sensor_product': i.split('|')[0],\n",
    "                                                '2_satellite_sensor_product': i.split('|')[1],\n",
    "                                                'gt3std_date': (candidate_pair[candidate_pair['ratio'] > 3*candidate_pair['ratio'].std()].index),\n",
    "                                                'correlation': candidate_pair[['main_count','secondary_count']].corr().main_count[1]}))\n",
    "filter_candidates_pd = pd.concat(filter_candidates_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_candidates_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confirm that the candidate_pair and results make sense. \n",
    "# High positive or negative zscore indicate how far away from the mean the sample is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pair[candidate_pair['ratio'] > 3*candidate_pair['ratio'].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pair[candidate_pair['ratio'] > 2*candidate_pair['ratio'].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_pair[candidate_pair['ratio'] > 1*candidate_pair['ratio'].std()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest doing this for each comparison for all combinations\n",
    "ax = candidate_pair.plot.scatter(x=candidate_pair['main_count'], y=['secondary_count'])\n",
    "ax.layout.xaxis = {    'anchor': 'y', 'domain': [0.0, 1.0], 'title': {'text': 'SENTINEL_3B_SLSTR_EUMETSAT'}}\n",
    "ax.layout.yaxis = {    'anchor': 'x', 'domain': [0.0, 1.0], 'title': {'text': 'NOAA 20_VIIRS_NASA2.0NRT'}}\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate dates should be excluded from the pairwise comparison results if they fail two tests:\n",
    "### 1. the ratio of counts for the pair exceeds a threshold of 3 standard deviations from the mean ratio (assumes that the ratio of hotspots will be reasonably stable)\n",
    "### 2. the threshold of 3 standard deviations from the mean ratio is also exceeded in at least 4 other pairwise comparisons where the master / target satellite_sensor_product \n",
    "\n",
    "### Use the results to drop candidates matching the time and satellite_sensor_product (Master) from the nearest match results - chances are they get excluded from the <5000m results anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date in filter_candidates_pd['gt3std_date'].unique():\n",
    "    for i in filter_candidates_pd['satellite_sensor_product'].unique():\n",
    "        candidate_product_count = filter_candidates_pd[(filter_candidates_pd['gt3std_date'] == date) & (filter_candidates_pd['satellite_sensor_product'] == i)].count()\n",
    "        if candidate_product_count[0] >= 3:\n",
    "            print(i, date, candidate_product_count[0])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_candidates_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample = satellite_sensor_product_daily_pd[satellite_sensor_product_daily_pd['pair'] == 'SUOMI NPP_VIIRS_LANDGATE|SENTINEL_3B_SLSTR_EUMETSAT']\n",
    "candidate_pair[['main_count', 'secondary_count']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "sample = satellite_sensor_product_daily_pd[satellite_sensor_product_daily_pd['pair'] == 'SENTINEL_3B_SLSTR_ESA|SENTINEL_3B_SLSTR_EUMETSAT']\n",
    "fig = px.line(sample, x=sample.index, y=sample['ratio'], color=sample['pair'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satellite_sensor_product_daily_list[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = nearest_points_ddf[(pd.to_datetime(nearest_points_ddf['datetime']).dt.date == np.datetime64('2019-12-24')) & (nearest_points_ddf['satellite_sensor_product'] == 'TERRA_MODIS_NASA6.03') & (nearest_points_ddf['2_satellite_sensor_product'] == 'SENTINEL_3B_SLSTR_ESA')]\n",
    "#hotspots_gdf.plot(column='satellite_sensor_product', legend=True, legend_kwds={'loc': 'upper right'}, figsize=(20, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "mapa = folium.Map([-26, 132],\n",
    "                  zoom_start=4,\n",
    "                  tiles='Stamen Terrain')\n",
    "points = folium.features.GeoJson(gpd.GeoSeries.from_wkt(subset.geometry).to_json())\n",
    "mapa.add_child(points)\n",
    "mapa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## Co-occurrence metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_alias = \"continental\"\n",
    "output_directory = processing_parameters[\"outdir\"]\n",
    "comparison_prefix = (\n",
    "    f\"{processing_parameters['start_date'].replace('-', '')}\"\n",
    "    f\"_{processing_parameters['end_date'].replace('-', '')}\"\n",
    "    f\"_{processing_parameters['start_time'].replace(':','')}\"\n",
    "    f\"_{processing_parameters['end_time'].replace(':','')}\"\n",
    "    f\"_{region_alias}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the nearest distance threshold between two hotspots to confine the analysis within the distance threshold.  \n",
    "dist_threshold = 5000  # units in meters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_ddf_dist_subset = nearest_points_ddf[nearest_points_ddf[\"dist_m\"] < dist_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_ddf_dist_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = util.pandas_pivot_table(\n",
    "    nearest_ddf_dist_subset,\n",
    "    index=[\"satellite_sensor_product\"],\n",
    "    columns=[\"2_satellite_sensor_product\"],\n",
    "    values=[\"count\"],\n",
    "    aggfunc={\"count\": np.sum}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator.to_csv(output_directory.joinpath(f\"{comparison_prefix}_matches_{dist_threshold}.csv\"))\n",
    "numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator = util.pandas_pivot_table(\n",
    "    nearest_points_ddf,\n",
    "    index=[\"satellite_sensor_product\"],\n",
    "    columns=[\"2_satellite_sensor_product\"],\n",
    "    values=[\"count\"],\n",
    "    aggfunc={\"count\": np.sum}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denominator.to_csv(output_directory.joinpath(f\"{comparison_prefix}_matches_count.csv\"))\n",
    "denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference of matched points closer than 5000m\n",
    "difference = denominator - numerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "difference.to_csv(output_directory.joinpath(f\"{comparison_prefix}_count_difference.csv\"))\n",
    "difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage of matched points closer than dist_threshold\n",
    "percentage = (numerator / denominator) * 100\n",
    "percentage = np.round(percentage, 2)\n",
    "##\n",
    "percentage = (numerator / denominator).style.format(\"{:.0%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percentage.to_csv(output_directory.joinpath(f\"{comparison_prefix}_percentage.csv\"))\n",
    "#percentage\n",
    "# Set seaborn styling for matrix\n",
    "cm = seaborn.color_palette(\"rocket_r\", as_cmap=True)\n",
    "s = percentage.background_gradient(cmap=cm)\n",
    "s.set_table_styles(\n",
    "    [dict(selector=\"th\",props=[('max-width', '200px')]),\n",
    "        dict(selector=\"th.col_heading\",\n",
    "                 props=[(\"writing-mode\", \"vertical-rl\"), \n",
    "                        ('transform', 'rotateZ(180deg)'),\n",
    "                        ])]\n",
    ")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum time between match points < dist_threshold\n",
    "timemax = util.pandas_pivot_table(\n",
    "    nearest_ddf_dist_subset,\n",
    "    index=[\"satellite_sensor_product\"],\n",
    "    columns=[\"2_satellite_sensor_product\"],\n",
    "    values=[\"timedelta\"],\n",
    "    aggfunc={\"timedelta\": np.max}\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timemax.to_csv(output_directory.joinpath(f\"{comparison_prefix}_max_time_matched_points.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timemaxtable = timemax.style.format(\"{:}\")\n",
    "cm = seaborn.color_palette(\"rocket\", as_cmap=True)\n",
    "s = timemaxtable.background_gradient(cmap=cm)\n",
    "s.set_table_styles(\n",
    "    [dict(selector=\"th\",props=[('max-width', '200px')]),\n",
    "        dict(selector=\"th.col_heading\",\n",
    "                 props=[(\"writing-mode\", \"vertical-rl\"), \n",
    "                        ('transform', 'rotateZ(180deg)'),\n",
    "                        ])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum time between match points < dist_threshold\n",
    "timemin = util.pandas_pivot_table(\n",
    "    nearest_ddf_dist_subset,\n",
    "    index=[\"satellite_sensor_product\"],\n",
    "    columns=[\"2_satellite_sensor_product\"],\n",
    "    values=[\"timedelta\"],\n",
    "    aggfunc={\"timedelta\": np.min}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timemin.to_csv(output_directory.joinpath(f\"{comparison_prefix}_min_time_matched_points.csv\"))\n",
    "timemin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average distance (m) between matched points < dist_threshold\n",
    "averagedist = util.pandas_pivot_table(\n",
    "    nearest_ddf_dist_subset,\n",
    "    index=[\"satellite_sensor_product\"],\n",
    "    columns=[\"2_satellite_sensor_product\"],\n",
    "    values=[\"dist_m\"],\n",
    "    aggfunc={\"dist_m\": np.mean}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averagedist = np.round(averagedist)\n",
    "averagedist.to_csv(output_directory.joinpath(f\"{comparison_prefix}_average_distance_{dist_threshold}m.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seaborn styling for matrix\n",
    "averagedisttable = averagedist.style.format(\"{:}\")\n",
    "cm = seaborn.color_palette(\"rocket\", as_cmap=True)\n",
    "s = averagedisttable.background_gradient(cmap=cm)\n",
    "s.set_table_styles(\n",
    "    [dict(selector=\"th\",props=[('max-width', '200px')]),\n",
    "        dict(selector=\"th.col_heading\",\n",
    "                 props=[(\"writing-mode\", \"vertical-rl\"), \n",
    "                        ('transform', 'rotateZ(180deg)'),\n",
    "                        ])]\n",
    ")\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#client.close() # close dask.distributed client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsetting for NSW Case Study\n",
    "for product in satellite_sensor_product:\n",
    "        unique_matched_hotspots = nearest_ddf_dist_subset[(nearest_ddf_dist_subset['satellite_sensor_product']==product)]\n",
    "        unique_matched_hotspots.drop(['Unnamed: 0', '2_latitude', '2_longitude',\n",
    "           '2_satellite', '2_sensor', '2_confidence', '2_power', '2_datetime',\n",
    "           '2_solar_day', '2_satellite_sensor_product', '2_geometry',\n",
    "           '2_solar_night', 'dist', 'dist_m', 'timedelta', 'count'],axis=1, inplace=True)\n",
    "        unique_matched_hotspots = unique_matched_hotspots.drop_duplicates()\n",
    "        unique_matched_hotspots = gpd.GeoDataFrame(unique_matched_hotspots,geometry=gpd.points_from_xy(unique_matched_hotspots.longitude, unique_matched_hotspots.latitude))\n",
    "        unique_matched_hotspots.cx[147:154, -38:-27].to_csv(output_directory.joinpath(f\"{product}_nsw_unique_within_5km.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
