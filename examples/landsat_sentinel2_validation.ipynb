{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S3VT Landsat and Sentinel 2 validation of hotspots - working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "This notebook demonstrates how to:\n",
    " \n",
    "From a candidate latitude longitude and solar_day:\n",
    "* determine if intersecting Landsat or Sentinel 2 ARD exists\n",
    "* apply the platform specific tests to determine if hotspots were detected in the vicinity 5km of hotspot\n",
    "* return number of pixel identified as hotspots\n",
    "* save a boolean file labelled with solar date of acquisition\n",
    "* as a secondary test perform a Normalized Burnt Ratio and return as a binary with solar date of acquisition\n",
    "    * find canidate dates within a time range of source hotspot\n",
    "        * find closest before date within tolerance (dNBR A)\n",
    "        * find closest after date within tolerance (dNBR B)\n",
    "        * candidate closest to source hotspot will be used for hotspot matching i.e. high resolution hotspot\n",
    "\n",
    "Assumptions:\n",
    "* reflectance values are scaled by 10000 i.e. 100% reflectance = 10000\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from pathlib import Path\n",
    "import datacube\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "sys.path.insert(0, '..')\n",
    "import src.hotspot_utils as util\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "from shapely import wkt\n",
    "from geopy.distance import distance\n",
    "\n",
    "import rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "    stream=sys.stdout,\n",
    ")\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "_LOG = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc = datacube.Datacube(app='validating_hotspots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be ignorant of the sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure sensor bands - #TODO implement sensor ignorance code here\n",
    "sensor_ignorance = {'s2msi':{'0.433-0.453': 'nbart_coastal_aerosol',\n",
    "                           '0.450-0.515': 'nbart_blue',\n",
    "                           '0.525-0.600': 'nbart_green',\n",
    "                           '0.630-0.680': 'nbart_red',\n",
    "                           '0.845-0.885': 'nbart_nir_1',\n",
    "                           '1.560-1.660': 'nbart_swir_2',\n",
    "                           '2.100-2.300': 'nbart_swir_3',\n",
    "                           'fmask': 'fmask'},\n",
    "                   'lsoli': {'0.433-0.453': 'nbart_band01',\n",
    "                           '0.450-0.515': 'nbart_band02',\n",
    "                           '0.525-0.600': 'nbart_band03',\n",
    "                           '0.630-0.680': 'nbart_band04',\n",
    "                           '0.845-0.885': 'nbart_band05',\n",
    "                           '1.560-1.660': 'nbart_band06',\n",
    "                           '2.100-2.300': 'nbart_band07',\n",
    "                           'fmask': 'fmask'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swaths = pd.read_pickle('../workdir_test1/swaths_154_113_20191101_20201008.pkl')\n",
    "swath_gdf = swaths[swaths['geometry'].is_valid == True]\n",
    "start_time_utc, end_time_utc = util.convert_solar_time_to_utc(154.0, 113.0, \"20:00\", \"03:00\")\n",
    "hotspots_pkl_file = ('../workdir_test1/all_hotspots_154_113_20191101_20201008_2000_0300.pkl')\n",
    "hotspots_gdf = pd.read_pickle(hotspots_pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf.set_index('datetime', inplace=True)#, drop=False\n",
    "hotspots_gdf = hotspots_gdf.between_time(start_time_utc , end_time_utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "product_availability = {}\n",
    "for index, row in hotspots_gdf.resample(\"D\", on='solar_night'):   \n",
    "    for product in hotspots_gdf['satellite_sensor_product'].unique():\n",
    "        results_list.append({'datetime': index, 'satellite_sensor_product': product, 'count': row[row['satellite_sensor_product'] == product].geometry.count()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_hotspot_count = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,product in enumerate(np.sort(daily_hotspot_count.satellite_sensor_product.unique(), axis = None)):\n",
    "    print(index, product)\n",
    "    daily_hotspot_count.loc[(daily_hotspot_count.satellite_sensor_product == product), 'available'] = \\\n",
    "    (daily_hotspot_count['count']/daily_hotspot_count['count'])*(index+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours= {'AQUA_MODIS_LANDGATE': 'lightsteelblue',\\\n",
    "          'AQUA_MODIS_NASA6.03': 'lavender',\\\n",
    "          'NOAA 20_VIIRS_LANDGATE': 'indianred',\\\n",
    "          'NOAA 20_VIIRS_NASA2.0NRT': 'lightcoral',\\\n",
    "          'NOAA-19_AVHRR_LANDGATE': 'grey',\\\n",
    "          'SENTINEL_3A_SLSTR_ESA': 'darkgreen',\\\n",
    "          'SENTINEL_3A_SLSTR_EUMETSAT': 'seagreen',\\\n",
    "          'SENTINEL_3B_SLSTR_ESA': 'lime',\\\n",
    "          'SENTINEL_3B_SLSTR_EUMETSAT': 'limegreen',\\\n",
    "          'SUOMI NPP_VIIRS_LANDGATE': 'firebrick',\\\n",
    "          'SUOMI NPP_VIIRS_NASA1': 'darkred',\\\n",
    "          'TERRA_MODIS_LANDGATE': 'navy', \\\n",
    "          'TERRA_MODIS_NASA6.03': 'blue'} \\\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# gca stands for 'get current axis'\n",
    "ax = plt.gca()\n",
    "ax.set_title('Hotspot data availability per product')\n",
    "\n",
    "for product in (daily_hotspot_count.satellite_sensor_product.unique()):\n",
    "    daily_hotspot_count[daily_hotspot_count['satellite_sensor_product'] == product].plot(kind='scatter',x='datetime',y='available',ax=ax, color=colours[product])\n",
    "\n",
    "ax.set(ylabel='product')\n",
    "ax.set(xlabel='date')\n",
    "    #ax.set_yticklabels(daily_hotspot_count.satellite_sensor_product)\n",
    "plt.yticks([1,2,3,4,5,6,7,8,9,10,11,12,13], colours.keys())\n",
    "\n",
    "\n",
    "plt.savefig('data_availability.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_hotspot_count.to_csv('dailyhotspotcount.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf[\"s2msi_rdnbr_gt_200\"] = \"\"\n",
    "hotspots_gdf[\"s2msi_pre_burn_time\"] = \"\"\n",
    "hotspots_gdf[\"s2msi_post_burn_time\"] = \"\"\n",
    "hotspots_gdf[\"s2msi_pre_burn_timedelta\"] = \"\"\n",
    "hotspots_gdf[\"s2msi_post_burn_timedelta\"] = \"\"\n",
    "hotspots_gdf[\"s2msi_pre_percent\"] = \"\"\n",
    "hotspots_gdf[\"s2msi_post_percent\"] = \"\"\n",
    "hotspots_gdf[\"s2msi_pre_hotspots\"] = \"\"\n",
    "hotspots_gdf[\"s2msi_post_hotspots\"] = \"\"\n",
    "hotspots_gdf[\"lsoli_rdnbr_gt_200\"] = \"\"\n",
    "hotspots_gdf[\"lsoli_pre_burn_time\"] = \"\"\n",
    "hotspots_gdf[\"lsoli_post_burn_time\"] = \"\"\n",
    "hotspots_gdf[\"lsoli_pre_burn_timedelta\"] = \"\"\n",
    "hotspots_gdf[\"lsoli_post_burn_timedelta\"] = \"\"\n",
    "hotspots_gdf[\"lsoli_pre_percent\"] = \"\"\n",
    "hotspots_gdf[\"lsoli_post_percent\"] = \"\"\n",
    "hotspots_gdf[\"lsoli_hotspot_percent\"] = \"\"\n",
    "hotspots_gdf[\"lsoli_pre_hotspots\"] = \"\"\n",
    "hotspots_gdf[\"lsoli_post_hotspots\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer candidate hotspot with 5 kilometre radius (or .05 degrees will do)\n",
    "def buffer_hotspot(lon, lat):\n",
    "    ul_lon = lon - 0.05\n",
    "    lr_lon = lon + 0.05\n",
    "    ul_lat = lat + 0.05\n",
    "    lr_lat = lat - 0.05\n",
    "    return ((ul_lon, lr_lon), (ul_lat, lr_lat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_date(firetime, days):\n",
    "    prefire_date = (firetime - np.timedelta64(days, \"D\")).astype(str)\n",
    "    postfire_date = (firetime + np.timedelta64(1, \"D\")).astype(str)\n",
    "    return(prefire_date, postfire_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_measurement_list(product):\n",
    "    measurement_list = []\n",
    "    for i in dc.list_products().name:\n",
    "        for j in dc.list_measurements().query('product == @i').name:\n",
    "            if i == product:\n",
    "                measurement_list.append([i, '--',j])\n",
    "    return(measurement_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially unambiguous active fire pixels\n",
    "def get_candidates(ds):\n",
    "    test1 = (((ds[sensor_ignorance[sensor]['2.100-2.300']] / ds[sensor_ignorance[sensor]['0.845-0.885']]) > 2.5) *\n",
    "             ((ds[sensor_ignorance[sensor]['2.100-2.300']]  - ds[sensor_ignorance[sensor]['0.845-0.885']]) > 3000) *\n",
    "             (ds[sensor_ignorance[sensor]['2.100-2.300']] > 5000))\n",
    "    # Unambiguous fire pixels\n",
    "    test2 = (((ds[sensor_ignorance[sensor]['1.560-1.660']] > 8000) *\n",
    "              (ds[sensor_ignorance[sensor]['0.433-0.453']] < 2000)) *\n",
    "             ((ds[sensor_ignorance[sensor]['0.845-0.885']] > 4000) +\n",
    "              (ds[sensor_ignorance[sensor]['2.100-2.300']] < 1000)).clip(min=0, max=1))\n",
    "    # other candidate fire pixels\n",
    "    test3 = (((ds[sensor_ignorance[sensor]['2.100-2.300']]/ds[sensor_ignorance[sensor]['0.845-0.885']]) > 1.8)*\n",
    "             (ds[sensor_ignorance[sensor]['2.100-2.300']]-ds[sensor_ignorance[sensor]['0.845-0.885']]  > 1700))\n",
    "    unambiguous = (test1 + test2 + test3).clip(min=0, max=1)\n",
    "    return(unambiguous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_kernel_array(y, x, array):\n",
    "    \n",
    "    T, Y, X = array.shape\n",
    "\n",
    "    ymin = y - 60\n",
    "    ymax = y + 60\n",
    "    xmin = x - 60\n",
    "    xmax = x + 60\n",
    "    \n",
    "    if ymin < 0:\n",
    "        ymin = 0\n",
    "    \n",
    "    if xmin < 0:\n",
    "        xmin = 0\n",
    "\n",
    "    if ymax > Y:\n",
    "        ymax = Y\n",
    "        \n",
    "    if xmax > X:\n",
    "        xmax = X\n",
    "\n",
    "    try:\n",
    "        outarray = array[0][:, ymin:ymax][xmin:xmax]\n",
    "    except:\n",
    "        outarray = np.nans((61,61), dtype=np.float64)\n",
    "    \n",
    "    return(outarray, (ymin, ymax, xmin, xmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test6(ds):\n",
    "    #6. ratio b7 b6 > 1.6\n",
    "    return((ds[sensor_ignorance[sensor]['2.100-2.300']]/ds[sensor_ignorance[sensor]['1.560-1.660']]) > 1.6 )         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oceans test\n",
    "#7. {b4 > b5 AND b5 > b6 AND b6 > b7 AND b1 - b7 < 0.2}\n",
    "def run_test7(ds):\n",
    "    test7 = ((ds[sensor_ignorance[sensor]['0.630-0.680']]>ds[sensor_ignorance[sensor]['0.845-0.885']])*\n",
    "             (ds[sensor_ignorance[sensor]['0.845-0.885']]>ds[sensor_ignorance[sensor]['1.560-1.660']])*\n",
    "             (ds[sensor_ignorance[sensor]['1.560-1.660']]>ds[sensor_ignorance[sensor]['2.100-2.300']])*\n",
    "             ((ds[sensor_ignorance[sensor]['0.433-0.453']]-ds[sensor_ignorance[sensor]['2.100-2.300']]) < 2000))\n",
    "\n",
    "    return(test7.clip(min=0, max=1))#.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water bodies test - comment - seems like  bad test / smoke complications?\n",
    "#AND\n",
    "#8. {(b3 > b2)\n",
    "def run_test8(ds):    \n",
    "    test8 = (ds[sensor_ignorance[sensor]['0.525-0.600']]>ds[sensor_ignorance[sensor]['0.450-0.515']])\n",
    "\n",
    "    return(test8.clip(min=0, max=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#OR\n",
    "#9. (b1 > b2 AND b2 > b3 AND b3 < b4)}.\n",
    "\n",
    "def run_test9(ds):\n",
    "    test9 = ((ds[sensor_ignorance[sensor]['0.433-0.453']]>ds[sensor_ignorance[sensor]['0.450-0.515']]) *\n",
    "            (ds[sensor_ignorance[sensor]['0.450-0.515']]>ds[sensor_ignorance[sensor]['0.525-0.600']])*\n",
    "            (ds[sensor_ignorance[sensor]['0.525-0.600']]<ds[sensor_ignorance[sensor]['0.630-0.680']]))\n",
    "  \n",
    "    return(test9.clip(min=0, max=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_watermasks(ds):\n",
    "    watermask=(run_test7(ds)+run_test8(ds)+run_test9(ds)).clip(min=0, max=1)\n",
    "    return(watermask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hotspots(ds):\n",
    "    # Find the candidates and perform context check\n",
    "    candidates = get_candidates(ds)\n",
    "    watermasks = get_watermasks(ds)\n",
    "    indices = np.where(candidates.data == 1)\n",
    "    swircandidates = (ds[sensor_ignorance[sensor]['2.100-2.300']].where(candidates.data == 0)).where(watermasks.data == 0)\n",
    "    nircandidates = (ds[sensor_ignorance[sensor]['0.845-0.885']].where(candidates.data == 0)).where(watermasks.data == 0)\n",
    "\n",
    "    test4 = (candidates*0)\n",
    "    test5 = (candidates*0)\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    while index < len(indices[1]):\n",
    "        y = indices[1][index]\n",
    "        x = indices[2][index]\n",
    "\n",
    "        #4. ratio between b7 b5 > ratio b7 b5 + max[3x std ratio b7 and b5, 0.8 ]\n",
    "        #AND\n",
    "        #5. b7 > b7 + max[3x std b7, 0.08]\n",
    "        #AND\n",
    "\n",
    "        swirkernel = get_context_kernel_array(y,x,ds[sensor_ignorance[sensor]['2.100-2.300']].data)[0]\n",
    "        nirkernel = get_context_kernel_array(y,x,ds[sensor_ignorance[sensor]['0.845-0.885']].data)[0]\n",
    "        #uncommented the above on 2 November\n",
    "        #swirkernel = get_context_kernel_array(y,x,swircandidates.data)[0]\n",
    "        #nirkernel = get_context_kernel_array(y,x,nircandidates.data)[0] \n",
    "\n",
    "        swir = ds[sensor_ignorance[sensor]['2.100-2.300']].data[0][y][x]\n",
    "        nir = ds[sensor_ignorance[sensor]['0.845-0.885']].data[0][y][x]\n",
    "\n",
    "        test4.data[0][y][x] = ((swir/nir) > (np.nanmean(swirkernel/nirkernel) + max(3*np.nanstd(swirkernel/nirkernel), 0.8))) \n",
    "        test5.data[0][y][x] = (swir > (np.nanmean(swirkernel) + max(3*np.nanstd(swirkernel), 0.08)))\n",
    "\n",
    "        #print(test4.data[0][y][x],(swir/nir), (np.nanmean(swirkernel/nirkernel) + max(3*np.nanstd(swirkernel/nirkernel), 0.8)))\n",
    "        #print(test5.data[0][y][x], swir,(np.nanmean(swirkernel) + max(3*np.nanstd(swirkernel), 0.08)) )\n",
    "        # Write values to new dimension\n",
    "        #print(index, y, x, get_context_kernel_array(y,x,ds[sensor_ignorance[sensor]['2.100-2.300']].data)[1])\n",
    "        index = index + 1\n",
    "    test6 = run_test6(ds)\n",
    "    t, y, z = np.where((candidates*(test4*test5*test6)).data == 1)\n",
    "    hotspots = len(y)\n",
    "    return(hotspots, (candidates*(test4*test5*test6)))\n",
    "#(candidates*(test4*test5*test6)).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nbr(ds):\n",
    "    swir = ds[sensor_ignorance[sensor]['2.100-2.300']]\n",
    "    nir = ds[sensor_ignorance[sensor]['0.845-0.885']]\n",
    "    return((nir - swir) / (swir + nir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rdnbr(pre_fire_image, post_fire_imag):\n",
    "    # Revitalising dNBR from NSW Govt \n",
    "    postfire_nbr = get_nbr(post_fire_image)\n",
    "    prefire_nbr = get_nbr(pre_fire_image)\n",
    "    dnbr = (prefire_nbr[0] - postfire_nbr[0])\n",
    "    # Scaling and offset as per NSW Govt algorithm\n",
    "    #rdnbr = ((dnbr/(np.sqrt(np.abs(prefire_nbr[0]))))*1000)\n",
    "    return((dnbr/(np.sqrt(np.abs(prefire_nbr[0]))))*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_list_rgb(image_list, label_list, fake_saturation):\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=len(image_list), figsize=(10,10))\n",
    "    column = 0\n",
    "    \n",
    "        \n",
    "    for image in image_list:\n",
    "        rgb = image.to_array(dim='color')\n",
    "        rgb = rgb.transpose(*(rgb.dims[1:]+rgb.dims[:1]))  # make 'color' the last dimension\n",
    "        #rgb = rgb.where((rgb <= fake_saturation).all(dim='color'))  # mask out pixels where any band is 'saturated'\n",
    "        rgb = (rgb-500)/fake_saturation  # scale to [0, 1] range for imshow       \n",
    "        \n",
    "        if len(image_list) == 1:\n",
    "            ax.axis('off')\n",
    "            ax.imshow(rgb[0])\n",
    "            ax.set_title(str(image.time.values)[2:12]+\" \\n \"+label_list[column],  wrap=True) \n",
    "        else:\n",
    "            ax[column].axis('off')\n",
    "            ax[column].imshow(rgb[0])\n",
    "            if column < 3:\n",
    "                ax[column].set_title(str(image.time.values)[2:12]+\" \\n \"+label_list[column],  wrap=True) \n",
    "            else:\n",
    "                ax[column].set_title(label_list[column],  wrap=True) \n",
    "            column = column+1\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing RGB\n",
    "#plot_list_rgb([ds.isel(time=0), ds.isel(time=0), ds.isel(time=0), ds.isel(time=0)], ['label', 'label','label', 'label'], 3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing\n",
    "#plot_list_rgb([pre_fire_image, post_fire_image, post_fire_image, post_fire_image], 3500)\n",
    "#plot_list_rgb([post_fire_image], 3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timedelta_from_ds(datacubeds, target_time):\n",
    "    timedelta = {}\n",
    "    index = 0\n",
    "    for i in list(datacubeds.time.data):\n",
    "        timedelta[index] = {\"time\": i, \"delta\": target_time - i}\n",
    "        index = index+1\n",
    "    return(pd.DataFrame.from_dict(timedelta).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portion_imaged(image):\n",
    "    t, y, x = np.where(image[sensor_ignorance[sensor]['2.100-2.300']] > 0)\n",
    "    t1, y1, x1 = np.where(image[sensor_ignorance[sensor]['2.100-2.300']] == 0)\n",
    "    return(len(y)/(len(y1)+len(y)), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portion_above_threshold(image, threshold):\n",
    "    y1, x1 = np.where(image)\n",
    "    y, x = np.where(image > threshold)\n",
    "    return(len(y)/(len(y1)+len(y)), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hotspots_gdf.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check you're getting the UTC times in the correct interval\n",
    "#for index, row in hotspots_gdf.resample(\"H\", on='solar_night'):\n",
    "#    print(index, len(row), row.datetime.max(), row.solar_night.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors_products = {'lsoli': ['ga_ls8c_ard_3'], 's2msi': ['s2a_ard_granule', 's2b_ard_granule']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('TM_WORLD_BORDERS_SIMPL-0.3.zip'):\n",
    "    !wget 'http://thematicmapping.org/downloads/TM_WORLD_BORDERS_SIMPL-0.3.zip'\n",
    "australia = gpd.GeoDataFrame.from_file('zip://TM_WORLD_BORDERS_SIMPL-0.3.zip')\n",
    "australia = australia[australia.NAME=='Australia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a given day, where geometry of Landsat or Sentinel 2 from datacube intersect\n",
    "# for a random set of hotspots, produce pre fire imagery, post fire imagery\n",
    "# hotspots and relativised normalised difference burnt area imagery.\n",
    "# Image selection is based on whether or not eligible pixels are above a percentage\n",
    "# threshold of the sample area\n",
    "join_results_list = []\n",
    "\n",
    "tries = 0\n",
    "for sensor in sensors_products.keys():\n",
    "    measurements = []\n",
    "    for measurement in sensor_ignorance[sensor]:\n",
    "        measurements.append(sensor_ignorance[sensor][measurement])\n",
    "    if sensor == 'lsoli':\n",
    "        buffer_days = 17\n",
    "    if sensor == 's2msi':\n",
    "        buffer_days = 12\n",
    "    \n",
    "    output_path = Path(sensor)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Intersect hotspots with sensor footprint from datacube daily query\n",
    "    \n",
    "    dataset_list=[]\n",
    "\n",
    "    for index_a, gdf_ra in hotspots_gdf.resample(\"D\", on='solar_night'):\n",
    "\n",
    "        # Get the most recent hotspot UTC time in the resampled period\n",
    "        image_utc_time = gdf_ra.datetime.max()\n",
    "        \n",
    "        # Search datacube for a period 24hrs extending beyond the end of the last sample\n",
    "        image_time_tuple = (str(image_utc_time), str(image_utc_time + np.timedelta64(24,'h')))\n",
    "\n",
    "        xtuple = (113, 154)\n",
    "        ytuple = (-10, -44)\n",
    "        query = {\n",
    "            'x': xtuple, \n",
    "            'y': ytuple,\n",
    "            'time': (image_time_tuple),\n",
    "            'measurements': measurements,\n",
    "            'output_crs': 'EPSG:3577',\n",
    "            'resolution': (-30, 30),\n",
    "            'group_by': 'solar_day'\n",
    "        }\n",
    "        \n",
    "        for product in sensors_products[sensor]:\n",
    "            datasets = dc.find_datasets(product=product, **query)\n",
    "            dataset_list.extend(datasets) \n",
    "        _LOG.info(f\"{product} product selected\")\n",
    "        geometry_list = []\n",
    "        for i in datasets:\n",
    "            geometry_list.append(i.extent.to_crs('epsg:4326'))\n",
    "        shapes = gpd.GeoDataFrame(gpd.GeoSeries(geometry_list))\n",
    "        shapes.columns = ['geometry']\n",
    "        shapes = shapes.set_crs('epsg:4326')\n",
    "        _LOG.info(f\"Shapes produced\")\n",
    "        \n",
    "\n",
    "        \n",
    "        # spatial join returning hotspots that intersect with datacube image bounds\n",
    "        #join = gpd.sjoin(gdf_ra, shapes)\n",
    "        join = gdf_ra.sjoin(shapes, how=\"inner\")\n",
    "        join = join.drop_duplicates()\n",
    "        _LOG.info(f\"Join successful\")\n",
    "        _LOG.info(f\"Hotspot time span from {join.datetime.min()} to {join.datetime.max()}\")\n",
    "        _LOG.info(f\"Datacube shapes time span from {image_time_tuple[0]} to {image_time_tuple[1]}\")\n",
    "        if len(join) > 0:\n",
    "            fig, ax = plt.subplots(figsize=(2,2))\n",
    "\n",
    "            ax.set_aspect('equal')\n",
    "            ax.axis('off')\n",
    "            australia.plot(ax=ax, color='white',  edgecolor='black')\n",
    "            shapes.plot(ax=ax, color='black')\n",
    "            join.plot(ax=ax, marker='o', color='red', markersize=5)\n",
    "            plt.show()\n",
    "        \n",
    "        # for each hotspot intersecting with the follow day, run hotspot detection\n",
    "        # Randomise selection of hotspots (maybe should randomise a set number for\n",
    "        # each hotspot product\n",
    "\n",
    "        choice = 0\n",
    "        hotspot_index_list = []\n",
    "        if len(join) > 0:\n",
    "            while choice < 20:\n",
    "                random_hotspot_index = random.choice(join.index)\n",
    "                if random_hotspot_index not in hotspot_index_list:\n",
    "                    hotspot_index_list.append(random_hotspot_index)\n",
    "                choice = choice+1\n",
    "\n",
    "        _LOG.info(f\"{hotspot_index_list} random indexes selected\")    \n",
    "\n",
    "        for hotspot_index in hotspot_index_list:\n",
    "            # Construct datacube query parameters from hotspot geometry\n",
    "\n",
    "            hotspot_lat = (join[(join.index == hotspot_index)].latitude.values[0])\n",
    "            hotspot_lon = (join[(join.index == hotspot_index)].longitude.values[0])\n",
    "            xtuple, ytuple = buffer_hotspot(hotspot_lon, hotspot_lat)\n",
    "            hotspot_utc_time = (join[(join.index == hotspot_index)].datetime.values[0])  \n",
    "            hotspot_time_tuple = buffer_date(hotspot_utc_time, buffer_days)\n",
    "            _LOG.info(f\"image UTC time {image_utc_time}\")\n",
    "            _LOG.info(f\"{[output_path,hotspot_index, xtuple, ytuple, hotspot_time_tuple, hotspot_utc_time]}\")\n",
    "            \n",
    "            try:\n",
    "                _LOG.info(f\"Attempting run_hotspot for candidate datacube ....\")\n",
    "                #pre_fire_image, post_fire_image, pre_percent, post_percent = run_hotspots(sensor_ignorance, sensor, measurements, output_path,hotspot_index, xtuple, ytuple, hotspot_time_tuple, hotspot_utc_time)\n",
    "                \n",
    "                \n",
    "                #### inexplicible failures so not using run_hotspots - presenting in line ####\n",
    "                \n",
    "                \n",
    "                query = {\n",
    "                    'x': xtuple, \n",
    "                    'y': ytuple,\n",
    "                    'time': (hotspot_time_tuple),\n",
    "                    'measurements': measurements,\n",
    "                    'output_crs': 'EPSG:3577',\n",
    "                    'resolution': (-30, 30),\n",
    "                    'group_by': 'solar_day'\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    dataset_list = []\n",
    "                    # Query datacube to find intersecting images for each hotspot\n",
    "                    for product in sensors_products[sensor]:\n",
    "                        datasets = dc.find_datasets(product=product, **query)\n",
    "                        dataset_list.extend(datasets)  \n",
    "\n",
    "                    ds = dc.load(datasets=dataset_list, **query)\n",
    "\n",
    "                    image_index = 0\n",
    "\n",
    "                    pd_timediff = get_timedelta_from_ds(ds, hotspot_utc_time) \n",
    "\n",
    "                    # Initialise variables\n",
    "                    pre_fire_candidate_delta = None\n",
    "                    post_fire_candidate_delta = None\n",
    "                    pre_fire_image = None\n",
    "                    post_fire_image = None\n",
    "                    _LOG.info(f\"{pd_timediff}\")\n",
    "                    _LOG.info(f\"Hotspot UTC date {hotspot_utc_time} and index {hotspot_index}\")\n",
    "\n",
    "                    while image_index < len(ds.time):\n",
    "                        _LOG.info(f\"{image_index+1} of {len(ds.time)} images being assessed ...\")\n",
    "\n",
    "                        # Load each image\n",
    "                        image = ds.isel(time=[image_index])[([sensor_ignorance[sensor]['2.100-2.300'],sensor_ignorance[sensor]['0.845-0.885'],sensor_ignorance[sensor]['0.450-0.515']])]#,'fmask'] )]\n",
    "\n",
    "                        # 1 == clear in FMask\n",
    "                        #### EXPERIMENTING WITH CLOUD MASK ####\n",
    "\n",
    "                        mask = ds.isel(time=[image_index])[('fmask')]\n",
    "                        mask = (mask*mask.values==1)\n",
    "\n",
    "                        image = image*mask\n",
    "                        #######################################\n",
    "                        # Add dataset time to filename\n",
    "\n",
    "                        portion, valid_count = get_portion_imaged(image)\n",
    "\n",
    "                        #pd_timediff[(pd_timediff.time != hotspot_utc_time)]\n",
    "                        _LOG.info(f\"{portion}, portion reported\")\n",
    "\n",
    "                        # Only use an image if at least 50% valid\n",
    "\n",
    "\n",
    "                        if (portion > 0.50):\n",
    "\n",
    "                            # If candidate on same day as hotspot, make it the post_fire_image\n",
    "                            _LOG.info(f\"{image_index+1} image of {len(pd_timediff)}\")\n",
    "                            if (image_index == pd_timediff[pd_timediff.delta.abs() == pd_timediff.delta.abs().min()].index):\n",
    "                                _LOG.info(f\"{image_index} - Post fire image found on hotspot day\")\n",
    "                                post_fire_candidate_delta = pd_timediff.iloc[[image_index]].delta.abs()\n",
    "                                post_fire_image = image\n",
    "                                post_index = image_index\n",
    "                                _LOG.info(f\"{len(pd_timediff[(pd_timediff.delta == pd_timediff.delta.abs())])} length of pd timediff\")\n",
    "                            else:\n",
    "                                # For candidates not on same day as hotspot, they can be split into pre and post hotspot \n",
    "\n",
    "                                # Handle pre hotspot candidates here\n",
    "                                if image_index in pd_timediff[(pd_timediff.delta == pd_timediff.delta.abs())].index:\n",
    "                                    # Do this for the first pass on pre_fire candidates\n",
    "                                    if pre_fire_candidate_delta is None:\n",
    "                                        _LOG.info(f\"Pre fire candidate found\")\n",
    "                                        pre_fire_candidate_delta = pd_timediff.iloc[[image_index]].delta.abs()\n",
    "                                        pre_fire_image = image\n",
    "                                        pre_index = image_index\n",
    "                                    # Do this for subsequent passes on pre_fire candidates\n",
    "                                    else:\n",
    "                                        # If subsequent pre fire candidates have a shorter delta, use them\n",
    "                                        if pre_fire_candidate_delta.values > pd_timediff.iloc[[image_index]].delta.abs().values:\n",
    "                                            pre_fire_candidate_delta = pd_timediff.iloc[[image_index]].delta.abs()\n",
    "                                            pre_fire_image = image\n",
    "                                            pre_index = image_index\n",
    "                                if image_index in pd_timediff[(pd_timediff.delta <  pd_timediff.delta.abs())].index:\n",
    "                                    if post_fire_candidate_delta is None:\n",
    "                                        # Consider post fire candidates\n",
    "                                        if (image_index == pd_timediff[(pd_timediff.delta < pd_timediff.delta.abs())].index):\n",
    "                                            post_fire_candidate_delta = pd_timediff.iloc[[image_index]].delta.abs()\n",
    "                                            post_fire_image = image\n",
    "                                            post_index = image_index\n",
    "                                    else:\n",
    "                                        if post_fire_candidate_delta.values > pd_timediff.iloc[[image_index]].delta.abs().values:\n",
    "                                            post_fire_candidate_delta = pd_timediff.iloc[[image_index]].delta.abs()\n",
    "                                            post_fire_image = image\n",
    "                                            post_index = image_index\n",
    "\n",
    "                        image_index = image_index + 1\n",
    "\n",
    "                except:\n",
    "                    result = '-'    \n",
    "\n",
    "                # Write results to file\n",
    "                if pre_fire_image is not None:\n",
    "                    \n",
    "                    _LOG.info(f\"Pre fire index = {pre_index}\")\n",
    "                    pre_fire_image.attrs.pop('grid_mapping', None)\n",
    "                    pre_fire_image.isel(time=0).rio.to_raster(output_path.joinpath(str(hotspot_index)+'_'+str(pre_fire_image.time[0].data)+'rgb.tif'))\n",
    "                    pre_hotspots, pre_hotspot_array = get_hotspots(ds.isel(time=[pre_index]))\n",
    "                    pre_hotspot_array.astype('int8').rio.to_raster(output_path.joinpath(str(hotspot_index)+'_'+str(pre_fire_image.time[0].data)+'hotspots.tif'))\n",
    "                    portion, pre_valid_count = get_portion_imaged(pre_fire_image)\n",
    "                else:\n",
    "                    _LOG.info(f\"No suitable Pre fire image found\")\n",
    "\n",
    "                if post_fire_image is not None:\n",
    "                    \n",
    "                    _LOG.info(f\"Post fire index = {post_index}\")\n",
    "                    post_fire_image.attrs.pop('grid_mapping', None)\n",
    "                    post_fire_image.isel(time=0).rio.to_raster(output_path.joinpath(str(hotspot_index)+'_'+str(post_fire_image.time[0].data)+'rgb.tif'))\n",
    "                    post_hotspots, post_hotspot_array = get_hotspots(ds.isel(time=[post_index]))\n",
    "                    post_hotspot_array.astype('int8').rio.to_raster(output_path.joinpath(str(hotspot_index)+'_'+str(post_fire_image.time[0].data)+'hotspots.tif'))\n",
    "                    portion, post_valid_count = get_portion_imaged(post_fire_image)\n",
    "                else:\n",
    "                    _LOG.info(f\"No suitable Post fire image found\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                ###########################\n",
    "\n",
    "                \n",
    "                \n",
    "                _LOG.info(f\"Hotspot result returned\")\n",
    "                rdnbr = get_rdnbr(pre_fire_image, post_fire_image)\n",
    "\n",
    "                rdnbr.astype('int16').rio.to_raster(output_path.joinpath(str(hotspot_index)+'_RdNBR.tif'))\n",
    "                \n",
    "                xr_post_hotspots = xr.Dataset({'1': post_hotspot_array,\n",
    "                                          '2': post_hotspot_array*0,\n",
    "                                          '3': post_hotspot_array*0})*3500\n",
    "                \n",
    "                xr_rdnbr = xr.Dataset({'1': rdnbr*3,'2': rdnbr*3,'3': rdnbr*3})\n",
    "                xr_rdnbr = xr_rdnbr.expand_dims({'time':1, })\n",
    "                xr_rdnbr = xr_rdnbr.transpose('time', 'y', 'x')\n",
    "                \n",
    "                plot_list_rgb([pre_fire_image, post_fire_image, xr_post_hotspots, xr_rdnbr ],\n",
    "                              ['pre fire RGB', 'post fire RGB', 'hotspots', 'RdNBR'], 3500)\n",
    "                \n",
    "                portion, valid_count = get_portion_above_threshold(rdnbr, 200)\n",
    "                _LOG.info(f\"{portion} rdnbr portion result\")\n",
    "                print(f\"{hotspot_index}, {portion} rdnbr portion result\")\n",
    "                \n",
    "                if (sensor == 's2msi'):  \n",
    "                    join.loc[hotspot_index, \"s2msi_rdnbr_gt_200\"] = portion\n",
    "                    join.loc[hotspot_index, \"s2msi_pre_burn_time\"] = pre_fire_image.time[0].data\n",
    "                    join.loc[hotspot_index, \"s2msi_post_burn_time\"] = post_fire_image.time[0].data\n",
    "                    join.loc[hotspot_index, \"s2msi_pre_burn_timedelta\"] = get_timedelta_from_ds(pre_fire_image, hotspot_utc_time).delta[0]\n",
    "                    join.loc[hotspot_index, \"s2msi_post_burn_timedelta\"] = get_timedelta_from_ds(post_fire_image, hotspot_utc_time).delta[0]\n",
    "                    join.loc[hotspot_index, \"s2msi_pre_percent\"] = pre_hotspots / pre_valid_count\n",
    "                    join.loc[hotspot_index, \"s2msi_post_percent\"] = post_hotspots / post_valid_count\n",
    "                    join.loc[hotspot_index, \"s2msi_pre_hotspots\"] = pre_hotspots\n",
    "                    join.loc[hotspot_index, \"s2msi_post_hotspots\"] = post_hotspots\n",
    "                    \n",
    "                if (sensor == 'lsoli'):\n",
    "                    join.loc[hotspot_index, \"lsoli_rdnbr_gt_200\"] = portion\n",
    "                    join.loc[hotspot_index, \"lsoli_pre_burn_time\"] = pre_fire_image.time[0].data\n",
    "                    join.loc[hotspot_index, \"lsoli_post_burn_time\"] = post_fire_image.time[0].data\n",
    "                    join.loc[hotspot_index, \"lsoli_pre_burn_timedelta\"] = get_timedelta_from_ds(pre_fire_image, hotspot_utc_time).delta[0]\n",
    "                    join.loc[hotspot_index, \"lsoli_post_burn_timedelta\"] = get_timedelta_from_ds(post_fire_image, hotspot_utc_time).delta[0]        \n",
    "                    join.loc[hotspot_index, \"lsoli_pre_percent\"] = pre_hotspots / pre_valid_count\n",
    "                    join.loc[hotspot_index, \"lsoli_post_percent\"] = post_hotspots / post_valid_count\n",
    "                    join.loc[hotspot_index, \"lsoli_pre_hotspots\"] = pre_hotspots\n",
    "                    join.loc[hotspot_index, \"lsoli_post_hotspots\"] = post_hotspots\n",
    "                    \n",
    "                # Collect results\n",
    "                join_results_list.append(join.loc[hotspot_index])\n",
    "    \n",
    "                print(f\"{hotspot_index} with {post_hotspots} {sensor} hotspots based on {str(join.loc[hotspot_index, 'satellite_sensor_product'])}\")\n",
    "            except:\n",
    "                print('either pre, post or both images not available for: ', hotspot_index)\n",
    "            \n",
    "            \n",
    "            # Uncomment the below for testing\n",
    "            #tries = tries + 1\n",
    "        \n",
    "            #if tries > 1:\n",
    "\n",
    "            #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather results - for some reason at times two records are returned - #TODO debug\n",
    "\n",
    "fixed_join_results_list = []\n",
    "for i in join_results_list:\n",
    "    if len(i) < 31:\n",
    "        fixed_join_results_list.append(pd.DataFrame(i.squeeze()).iloc[[0]])\n",
    "    else:\n",
    "        fixed_join_results_list.append(pd.DataFrame(i.squeeze()).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2msi_results_list = []\n",
    "lsoli_results_list = []\n",
    "for i in fixed_join_results_list:\n",
    "    if i.s2msi_pre_burn_time.values == \"\":\n",
    "        lsoli_results_list.append(i)\n",
    "    else:\n",
    "        s2msi_results_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2msi_results_gpd = gpd.GeoDataFrame(pd.concat(s2msi_results_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsoli_results_gpd = gpd.GeoDataFrame(pd.concat(lsoli_results_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filehandler = open('fixed_join_results_list.pkl', 'wb') \n",
    "pickle.dump(fixed_join_results_list, filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, product in lsoli_results_gpd.groupby('satellite_sensor_product'):\n",
    "    candidate_hotspots = len(product)\n",
    "    highres_hotspots = len(product[product['lsoli_post_hotspots'] > 0])\n",
    "    percent_confirmed = str((highres_hotspots/candidate_hotspots)*100)[:5]+\"%\"\n",
    "    print(index, candidate_hotspots, highres_hotspots, percent_confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, product in s2msi_results_gpd.groupby('satellite_sensor_product'):\n",
    "    candidate_hotspots = len(product)\n",
    "    highres_hotspots = len(product[product['s2msi_post_hotspots'] > 0])\n",
    "    percent_confirmed = str((highres_hotspots/candidate_hotspots)*100)[:5]+\"%\"\n",
    "    print(index, candidate_hotspots, highres_hotspots, percent_confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open('lsoli_results_gpd.pkl', 'wb') \n",
    "pickle.dump(lsoli_results_gpd, filehandler)\n",
    "lsoli_results_gpd.to_csv('lsoli_results_gpd.csv')\n",
    "filehandler = open('s2msi_results_gpd.pkl', 'wb') \n",
    "pickle.dump(s2msi_results_gpd, filehandler)\n",
    "s2msi_results_gpd.to_csv('s2msi_results_gpd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "australia.plot(ax=ax, color='white',  edgecolor='black')\n",
    "s2msi_results_gpd.plot(ax=ax, marker='o', color='blue', markersize=2)\n",
    "lsoli_results_gpd.plot(ax=ax, marker='o', color='red', markersize=2)\n",
    "plt.savefig('random_ls_s2.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "451432e065124f0787b3e7a25e3e0d41": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletZoomControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "zoom_in_text",
        "zoom_in_title",
        "zoom_out_text",
        "zoom_out_title"
       ]
      }
     },
     "a8eff2e4864346ca8959cb79bdf3f9ab": {
      "model_module": "jupyter-leaflet",
      "model_module_version": "^0.11.1",
      "model_name": "LeafletAttributionControlModel",
      "state": {
       "_model_module_version": "^0.11.1",
       "_view_count": null,
       "_view_module_version": "^0.11.1",
       "options": [
        "position",
        "prefix"
       ],
       "position": "bottomright",
       "prefix": "Leaflet"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
